# Things to do for 2023

- Remove non-pipe stuff from lab1 and just use the pipe from the start.
- Potentially reduce content in lab1 a little - quite long to go through.
- Lecture 4 way too long. Got bogged down in the y ~ x1 + x2 example. Needs optimising.
- Add ROC curves to classification notes. Need a section on "making a decision from a classification model"?


# ROC curves

Regarding ROC curves, these are largely used in classification. I don't have any material on that in the notes at this stage - it was one thing I was hoping to include, but I don't think we'll have time to cover it this year (I am rewriting the course, so some things unfortunately don't make it in).

Essentially the idea is where you have a binary outcome variable (i.e. two classes, e.g. positive/negative on a covid test for example) and instead of just predicting the class, many classification models predict the *probability* of being in a class. So you might have a 60% probability of being a positive for example (and thus 40% chance of being a negative). When we assign a class, we could just take 50% as the threshold for allocation to a positive or negative. But, we could instead prefer some other threshold - e.g. maybe we want P(+ve) > 0.65 before we say the individual is likely positive. The threshold we choose then defines which individuals that are allocated to each class, and, if we have the true class information (e.g. from an artificial test set) we can then cross-tabulate to work out how often we got true positives assigned as 'positive' and true positives assigned as 'negative' and so on. These then tell us the sensitivity and specificity of the classification model. There's usually a trade-off between these - you can get 100% sensitivity just by saying "everyone is positive". You can get 100% specificity by saying "everyone is negative". As the threshold varies, you vary between these two extremes.

The ROC is basically the curve you get of the pairs sensitivity, specificity (usually one-minus sensitivity) as you vary the threshold. The idea is you can optimise both sensitivity and specificity by choosing a threshold that maximises both.

You can use `yardstick::roc_curve` to compute the ROC curve:

https://yardstick.tidymodels.org/reference/roc_curve.html

You pass it the true class, along with the probability column (e.g. from parsnip::predict or parsnip::augment) - don't worry if these packages don't make sense yet - we'll get to them next week, and in more detail when we do classification in week 7. smile