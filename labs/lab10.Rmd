---
title: 'Workshop 10: Clustering'
output: html_document
---

<style>
.answer {
  background-color: #d0ebff;
  padding: 20px;
}

.answer pre {
  background-color: #d0ebff;
}
</style>

In this workshop we'll be looking at clustering data. Clustering is an unsupervised learning technique - there is no known truth, and we're instead trying to group the observations into a fairly small number of clusters such that all observations in a clusters are closely related, while observations from different clusters differ.

The techniques we'll cover include:

- Hierarchical clustering
- k-means clustering
- Partitioning around Medoids
- Silhouette measure

## The 2020 New Zealand General Election

In this exercise we'll look at the results from the 2020 New Zealand general election, and look at how the 72 electorates (regions) cluster together based on the party and candidate votes, or both combined.

In New Zealand general elections there are 65 'General' electorates (regions) and 7 Maori electorates. Each voters is assigned to one of these electorates, and has two votes: they vote for the political party they wish to represent them in parliament, and in addition vote for the person (candidate) they wish to represent them in their electorate.

Generally, the candidate they vote for in an electorate is also in a political party, but there is no need for an individual to vote for the same party across both their votes - indeed, vote splitting, where one votes for a person from a party other than the one you vote for is common and encouraged in some electorates.

For more information about voting in New Zealand see here:

https://elections.nz/democracy-in-nz/what-is-new-zealands-system-of-government/

We'll be usig data from the 2020 general election, which we read in below.

```{r, message=FALSE}
library(tidyverse)
library(broom)
library(ggdendro)
library(cluster)
library(recipes)

party  <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/nzelect/2020_party.csv")
candidate <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/nzelect/2020_candidate.csv")
```


Let's take a look at the `party` data. You should notice you have the percentage vote for each party (in separate columns) by the electorate (in rows). Summarise how each party does on average across the electorates.

```{r}
party |>
  summarise(across(-Electorate, mean))
```

Now, we'll make a Principal Component Analysis plot of the data to see if there are any clear 'clusters' of points.

```{r}
library(factoextra)

party |>
  column_to_rownames("Electorate") |> 
  prcomp() |> 
  fviz_pca_ind(repel=T, labelsize=2)
```

### Try it yourself

**1. Produce a long-form dataset `party_long` by pivoting the vote columns into `Party` and `PartyVote` columns. Use this to produce boxplots of the `PartyVote` by `Party` to show the variation across the electorates.**

:::{class="answer"}

:::

**2. Produce a long-form version of the `candidate` data, and combine both into a `all_long` dataset. Use this to do a scatterplot of the person vote versus the party vote, coloured by Party.**

:::{class="answer"}

:::

---

Next, we'll next look at clustering the `party` data using hierarchical clustering with average linkage.

```{r}
party.hc1 <- party |>
  column_to_rownames("Electorate") |>
  dist() |>
  hclust(method='average')

ggdendrogram(party.hc1)
```

### Try it yourself

**1. Perform a hierarchical clustering with single and/or complete linkage. Show the dendrograms.**

:::{class="answer"}

:::

**2. Which electorates tend to cluster together?**

:::{class="answer"}

:::

---


Let's see how many clusters look sensible for this dataset.

```{r}
party |> 
  column_to_rownames(var="Electorate") |> 
  fviz_nbclust(hcut, method = "silhouette")
```

Note that this is based on the 'Ward' linkage criterion -- that is, the criterion used to measure the distances for groups of objects -- because it is the default for the `hcut` function. "Ward.D2" usually gives similar results to average linkage.

Now let's make the cut at 6 (the result for 3, which is favoured by the silhouette metric, is a bit dull). 

Here's one way to do it, with the original `party.hc1` object we made.

```{r}
party.cl1 <- party |> 
  mutate(Cluster = party.hc1 |> cutree(k=6))
```

Here's another option using the `hcut` function, which does the distance matrix, the cluster analysis, and the cutting all in one step.

```{r}
party_hcut6_ave <- party |> 
  column_to_rownames("Electorate") |> 
      hcut(hc_method="average", k=6)

party.cl1 <- party |> 
  mutate(Cluster = party_hcut6_ave$cluster)

party.cl1 |> group_by(Cluster) |>
  summarise(across(-Electorate, mean), size=n())

```

We can display this information with a radar chart.

```{r, fig.height=8, fig.width=12}
library(ggradar)
library(scales)

party.cl1 |> 
  # rescale by maximum of each party
  mutate_at(vars(Act:Other), rescale) |> 
  group_by(Cluster) |>
  summarise(across(-Electorate, mean), size=n()) |> 
  # make cluster labels with sizes
  mutate(Cluster = paste0("Cluster ", Cluster," (n=",size,")")) |> 
  select(-size) %>% 
  ggradar(legend.position = "none") +
  facet_wrap(~Cluster) 
```

We can see that there is one dominant Cluster 3, containing 42 electorates that are hot on National and Act. Cluster 1 is strong for the Greens, Act. Cluster 2 are balanced between Labour, National, and Act. Cluster 4 contains three electorates that are extremely strong for Labour. Cluster 5 is one very Green electorate. Cluster 6 comprises the Maori electorates, which all tend to have a much higher vote for the Maori and Other parties.

Let's look at a Principal Components Analysis plot with the clusters overlaid.

```{r}
library(factoextra)

party |> 
  column_to_rownames("Electorate") |> 
  hcut(k=6, hc_method="average") |> 
  fviz_cluster(data=party, repel=T, labelsize = 6, 
               show.clust.cent = F, ggtheme=theme_bw(), main="")
```

Two notes of warning here:

1. The cluster labels may be different when we use a different function (`hcut()` vs `hclust() |> cutree()`).

2. The PCA axes are different to the first PCA plot above. I suspect they use two different methods, one of which automatically rescales the variables.


### Try it yourself

**1. Do the linkage methods result in the same clusters?**

:::{class="answer"}

:::

**2. Do the analysis with average linkage using the `candidate` vote. How do the electorates cluster? Choose a suitable number of clusters based on the dendogram and/or silhouette measure and then look into which electorates fall in each cluster.**

:::{class="answer"}

:::

---

We'll now combine the two datasets so we can look at them together:

```{r}
all <- party |> 
  rename_with(~ paste("Party", ., sep=":"), .cols=-Electorate) |>
  left_join(
    candidate |> rename_with(~ paste("Cand", ., sep=":"), .cols=-Electorate),
    by = 'Electorate'
  )
```

### Try it yourself

**1. Using the combined voting data `all`, perform hierarchical clustering and investigate which electorates tend to vote similarly when both party and candidate votes are considered.**

:::{class="answer"}

:::

---

## Clustering synthetic data

In this exercise we have 9 numerical measurements (labelled A through I) from a synthetic data set. Our goal is to group the observations with common characteristics together.

```{r, message=FALSE}
synth <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/syn-cluster.csv")
```

### Try it yourself

**1. Start by using hierarchical clustering to look at this data. How many clusters look sensible?**

:::{class="answer"}

:::

---

Next we'll use k-means clustering. The following code fits k-means for k = 1 through 10:

```{r}
set.seed(1234)
km_all <- tibble(k = 1:10) |>
  mutate(
    model = map(k, ~ kmeans(synth, centers = ., nstart = 20)),
    tidied = map(model, glance)
  ) |>
  unnest(tidied)
km_all
```

### Try it yourself

**1. Produce a plot of the within sum of squares against $k$. How many are clusters do you think are optimal?**

:::{class="answer"}

:::

**2. Check the scales on each of the variables in `synth`. You should notice they are different!**

:::{class="answer"}

:::

---

We can rescale the data using a `recipe` as usual:

```{r}
scaling <- recipe(synth) |>
  step_normalize(everything()) |>
  prep(synth)

synth.scale <- bake(scaling, synth)
```

### Try it yourself

**1. Repeat the clustering procedure. How many clusters make sense now?**

:::{class="answer"}

:::

---

Eight clusters seems sensible. We can confirm this by looking at the silhouettes of 6, 7, and 8 clusters. 

```{r}
synth.scale |> 
  kmeans(6, nstart=50) |> 
  pluck("cluster") |> 
  cluster::silhouette(dist(synth.scale)) |> 
  fviz_silhouette(label = T, print.summary = F) 

synth.scale |> 
  kmeans(7, nstart=50) |> 
  pluck("cluster") |> 
  cluster::silhouette(dist(synth.scale)) |> 
  fviz_silhouette(label = T, print.summary = F) 

synth.scale |> 
  kmeans(8, nstart=50) |> 
  pluck("cluster") |> 
  cluster::silhouette(dist(synth.scale)) |> 
  fviz_silhouette(label = T, print.summary = F) 
```

We can extract the silhouette information by re-fitting using `pam`:

```{r}
clust8 <- pam(synth, k=8)
clust8.sil <- clust8 |> pluck('silinfo', 'widths') |> as_tibble()
```

### Try it yourself

**1. Create boxplots of the silhouette width by cluster using `clust8.sil`.**

:::{class="answer"}

:::

**2. Repeat this for k=7. You might wish to combine the plots into one by using `bind_rows()` to  produce a single plot.**

:::{class="answer"}

:::

**3. Look at the results of your preferred clustering by using `augment()` from `broom` to append the cluster number to the data. With these data, making boxplots and radar plots for each variable by the cluster will be enough to see how clearly separate they are!**

:::{class="answer"}

:::