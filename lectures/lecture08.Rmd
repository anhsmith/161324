---
title: 'Lecture 8'
subtitle: 'Logistic Regression'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(ggbeeswarm)
library(skimr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

swiss.train <- read_csv("../data/swiss-train.csv") |>
  mutate(type = factor(type))
swiss.test <- read_csv("../data/swiss-test.csv") |>
  mutate(type = factor(type))
```

## Logistic Regression

Linear Discriminant Analysis (LDA), Kernel Discriminant Analysis (KDA) and Naive Bayes are all *generative* classifiers. They model $\mathsf{P}(j|\mathbf{x})$ by first learning $f_j(\mathbf{x})$ and $\pi_j$.

The alternative approach is to model posterior probabilities $\mathsf{P}(j | \mathbf{x})$ directly. Methods that do so are described as *discriminatory* classifiers.

One approach is to assume that $\mathsf{P}(j | \mathbf{x})$ takes a particular parametric form, then estimate the parameters from training data.

One such technique is **logistic regression**.

---

## Generalised Linear Models

Logistic regression is one of a class of methods known as **generalised linear models** (GLMs).     

GLMs are closely related to linear regression, with two important generalisations.

- The response (target) variable $y$ need not be distributed according to a normal distribution. We can assume a Bernoulli, Poisson, Negative Binomial, Gamma, or any other member of the "exponential" family of distrbutions for $y$.

- We may fit a linear model to the mean of $y$ after applying a transformation. The form of the tranformation is called the "link function".

--

The standard linear model: $\ \ \ \ y \sim \text{Normal}(\mu, \sigma^2) \ \ \ \ \ \ \ \mu = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p$

A generalised linear model: $\ \ \ \ y \sim \phi(\theta) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g(\theta) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p$

where $\phi$ is the assumed distribution for the data given parameters $\theta$, and $g()$ is a link function (e.g., the logit or log function).

--

The standard linear model is just a GLM where $\phi$ is the Normal distribution and $g()$ is the identity function.

---


## Introduction to Logistic Regression

Logistic regression can be used when the response variable is **binary** (i.e., it has two classes).     
The two classes, $j = \{1,2\}$, are labelled '0' and '1', respectively. 

A logistic regression model usually described in terms of $p(\mathbf{x})=\mathsf{P}(y=1 | \mathbf{x})$, where
- $p(\mathbf{x})$ is a function of predictor variable(s) $\mathbf{x}$, and
- $\mathsf{P}(y=1 | \mathbf{x})$ is the the probability that the response variable $y$ takes the value $1$ given the value of the predictor variable(s) $\mathbf{x}$.

Later, we can extend this model to more than two classes.

---

## Introduction to Logistic Regression

We want to estimate posterior probability that $y = 1$ (equivalent to $j=2$);      
that is, $p(\mathbf{x}) =  \mathsf{P}(y=1 | \mathbf{x}) = \mathsf{P}(j=2 | \mathbf{x})$.

> An aside: beware of possible confusion regarding the notation $p$ between:
- $p(\mathbf{x}) = \mathsf{P}(y=1 | \mathbf{x}) = \mathsf{P}(j=2 | \mathbf{x})$, and 
- $p$ as the number of predictors.

Given that this probability is a number, why not just use linear regression?

- A linear regression model would be: $p(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$

- A probability must be within the range $[0,1]$.     
*There's nothing in the linear regression model to confine predictions to this range*.

- For example, suppose we have single predictor variable, and fitted model is $\hat p(x) = 0.3 + 0.1 x$.     
If $x < -3$ or $x > 7$, we're outside the range $[0,1]$.

---

## Introduction to Logistic Regression

A solution is to use a type of sigmoidal link function that you met when looking at neural networks.

We can model probability $p(\mathbf{x})$ via the **logistic** (or sigmoidal) function: $$p(\mathbf{x}) = \frac{1}{1 + e^{-g(\mathbf{x})}}$$ where $g(\mathbf{x})$ is a **linear predictor**, given by
    $$g(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p.$$

*Importantly, the transformation of the linear predictor via the logistic function ensures that $p(\mathbf{x})$ stays within the range $[0,1]$ .*

The inverse of the logistic function is the **logit** or log-odds function
    $$g(\mathbf{x}) = \mathsf{logit}(p(\mathbf{x})) = \log\left(\frac{p(\mathbf{x})}{1-p(\mathbf{x})}\right).$$

---

## Logistic and logit curves

```{r logistic, message=FALSE, echo=FALSE, fig.dim=c(7, 3)}
g1 <- ggplot() +
  geom_function(fun = ~1/(1+exp(-.x)), xlim=c(-5, 5)) +
  labs(x = expression(eta),
       y = expression(p),
       title = "logistic function")

g2 <- ggplot() +
  geom_function(fun = ~log(.x/(1-.x)), xlim=c(0.00001, 0.99999)) +
  labs(x = expression(p),
       y = expression(eta),
       title = "logit function")

g1+g2
```

Any value of linear predictor $-\infty \le \eta \le \infty$
    corresponds to a feasible probability $0 < p < 1$.

---

## Formulation of logistic regression

$$\begin{aligned}
y_i &\sim \mathsf{Bernoulli}(p(\mathbf{x}_i))\\
\mathsf{logit}(p(\mathbf{x}_i)) &= \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}.\end{aligned}$$
where

- $y_i$ is the response (target) variable for $i$th observation (record).

- The $\mathsf{Bernoulli}$ distribution produces $1$ with probability $p$ and $0$ with probability $1-p$.

- $p(\mathbf{x}_i) =\mathsf{P}(y_i = 1 | \mathbf{x}_i)$, where $\mathbf{x}_i$ is vector of
    predictors (features) for $i$th observation.

-   Predictors may be quantitative, or factors coded as 0/1 indicator variables     
(just like for ordinary linear regression models).

-   $\beta_1, \ldots, \beta_p$ are unknown regression parameters that can be estimated from training data.

--

Two key changes to the ordinary regression model: 
- The output of the linear predictor is mapped onto the valid probability range $[0,1]$ via the logistic function. In GLMs, such a "mapping" function is generally called a "link function".
- The data (given the parameters) are not assumed to be normally distributed; instead, they are assumed to come from a *Bernoulli* distribution (which can only produce $0$s and $1$s).

---

## Fitting logistic regression models

To fit the model, we need a criterion by which to choose the values of the parameters estimating parameters $\beta_1, \ldots, \beta_p$. 

With ordinary regression, we can just minimise the sum of squared errors $\sum_{i=1}^n (y_i - \hat{y}_i)^2$.     
This doesn't work well when $y_i$ are just a bunch of $0$s and $1$s, and $\hat{y}_i$ are probabilities.

Instead, we estimate parameters by maximising the **likelihood** function.

The likelihood is just the joint probability of observing the data points $y_i$     
given they are assumed to come from a Bernoulli distribution with estimated probabilities $\hat{p}_i$.

The estimated probabilities $\hat{p}_i$ are just a function of the predictors and parameters, $\{ \mathbf{x}_i, \beta_1, \ldots, \beta_p \}$ .     
The predictors $\mathbf{x}_i$ are fixed, so parameters $\{ \beta_1, \ldots, \beta_p \}$ are chosen to maximies the joint probability of the observed data $y_i$.

---

## Fitting logistic regression models

The likelihood for logistic regression is: 
    $$\begin{aligned}L(\beta_1, \ldots, \beta_p) &= \prod_{i=1}^n \mathsf{P}(y_i | \mathbf{x}_i, \beta_1, \ldots, \beta_p) \\ &= \prod_{i=1}^n p(\mathbf{x}_i, \mathbf{\beta})^{y_i} (1 - p(\mathbf{x}_i, \mathbf{\beta}))^{1-y_i}\end{aligned}$$
    
where 
     $p(\mathbf{x}_i, \mathbf{\beta})$ emphasises dependence of $p(\mathbf{x}_i)$ of $\mathbf{\beta} = (\beta_1, \ldots, \beta_p)^\mathsf{T}$.

In practice, due to mathematical simplicity, we maximise the **log-likelihood**
    $$\ell(\mathbf{\beta}) = \log L(\mathbf{\beta}) = \sum_{i=1}^n y_i \log(p(\mathbf{x}_i, \mathbf{\beta})) + (1-y_i) \log(1 - p(\mathbf{x}_i, \mathbf{\beta})).$$

The set of parameter values $\hat{\mathbf{\beta}}$ that maximise $\ell(\mathbf{\beta})$ is called the
    **maximum likelihood estimate** (MLE).

There is no closed form solution; we fit this model using algorithmic techniques.

> More on GLMs in 161.327 and 161.331.

---

## Fitting logistic regression models in R

To fit logistic (and other) generalised linear models in R, we use the function 

    glm(formula, family, data)

where 

- `formula` is the model formula, e.g.Â `y ~ x1 + x2`    
(make sure `y` is a binary 1/0 variable or a two-level factor),

-   `family` specifies the response distribution; for logistic regression specify `family=binomial`,

-   `data` is the data frame to use (optional).

---

## Logistic regression for the Swiss banknotes data

.left-code[

```{r swissglm, eval=FALSE}
swiss.glm.1 <-
  glm(type ~ margin,
      family=binomial,
      data=swiss.train)
summary(swiss.glm.1)
```

Notes:
-   `type` is a factor with two levels. `forged` is the first level
    so represents $y=0$. `genuine` represents $y=1$.

-   The coefficient for `margin` is negative, so $\mathsf{P}(\mathsf{genuine} | \mathsf{margin})$
    decreases as `margin` increases.

-   **Deviance** is a function of the log likelihood (the criterion used, instead of sum of squares, to fit the generalised linear model).
]

.right-plot[
```{r, ref.label='swissglm', echo=FALSE}
```
]

```{r, echo=FALSE}
beta0 = round(coef(swiss.glm.1)[1], 4)
beta1 = round(coef(swiss.glm.1)[2], 4)
```

---

.left-code[
## Logistic Regression for the Swiss Banknotes

```{r, ref.label='swissglm', eval=FALSE}
```

The probability of being `genuine` falls below $0.5$ when $\mathsf{margin} > 9.25$mm.
]

.right-plot[
```{r, echo=FALSE}
new_data <- tibble(margin = seq(6,12.5,length=400))
swiss.pred <- swiss.glm.1 |> broom::augment(newdata=new_data,
                        type.predict='response')
xint <- swiss.pred |> mutate(lt = .fitted-0.5) |>
  slice_min(abs(lt), n=2) |> lm(margin ~ lt, data=_) |>
  predict(newdata=data.frame(lt=0))

ggplot(swiss.pred) +
  geom_hline(yintercept=0.5, col='gray60', linetype='dashed') +
  geom_vline(xintercept=xint, col='gray60', linetype='dashed') +
  geom_line(mapping=aes(x=margin, y=.fitted)) +
  labs(y = "P(genuine | margin)")
```
]

---

## Selection of predictor variables

If we have $p$ predictors available, we do not necessarily want to use all of them.

We may get a much better predictive model (better bias-variance trade off) by only using some.

One option is to compare a variety of logistic regressions on validation data.

A computationally cheap alternative is to do backwards variable selection based on the Akaike Information Criterion (AIC).

---

## More Logistic Regression for Banknotes

.left-code[

```{r swissstep, eval=FALSE}
swiss.glm.2 <- glm(type ~.,
                   family=binomial,
                   data=swiss.train)
step(swiss.glm.2)
```

-   `swiss.glm.2` is a logistic regression based on $p=2$ predictor
    variables: `margin` and `diagonal`.

-   Would the model be better with just one of those variables?

-   According to AIC criterion, model with both variables has lowest AIC
    (13.260).

-   Dropping either `margin` or `diagonal` results in higher AIC scores, so best to keep both predictors.
]

.right-plot[
```{r, ref.label="swissstep", echo=FALSE}
```
]

---

class: middle, inverse

# Classification with logistic regression

---

## Logistic regression for classification

Logistic regression provides a means of estimating the probability of a 1 outcome in a binary response based on predictor variables.

It can hence be used for two group classification.

Let $\mathbf{x}_0 = (x_{01}, \ldots, x_{0p})^\mathsf{T}$ be feature vector for a test case.

Logistic regression provides estimate of $p(\mathbf{x}_0) = \mathsf{P}(y=1 | \mathbf{x}_0)$.

Suppose (as usual) that we assign test case to class with highest
    posterior probability.

Then assign to class indicated by $y=1$ if and only if $\mathsf{P}(y=1 | \mathbf{x}_0) \ge \tfrac12$.

---

## Logistic regression and classification rules

Recall that $$p(\mathbf{x}_0) = \frac{1}{1 + e^{-g(\mathbf{x}_0)}}$$ 
where
    $g(\mathbf{x}_0) = \mathsf{logit}(p(\mathbf{x}_0)) = \beta_0 + \beta_1 x_{01} + \cdots + \beta_p x_{0p}$.

Classification to class $y=1$ occurs if and only if
    $$\begin{aligned}\mathsf{P}(y=1 | \mathbf{x}_0) \ge \tfrac12 &\Leftrightarrow& \tfrac{1}{1 + e^{-g(\mathbf{x}_0)}} \ge \tfrac12 \\ &\Leftrightarrow& e^{-g(\mathbf{x}_0)} \le 1 \\ &\Leftrightarrow& g(\mathbf{x}_0) \ge 0 \\ &\Leftrightarrow& \beta_0 + \beta_1 x_{01} + \cdots + \beta_p x_{0p} \ge 0 \end{aligned}$$

In practice, we replace parameters $\beta_0, \ldots, \beta_p$ by their maximum likelihood estimates, $\hat{\beta}_0, \ldots, \hat{\beta}_p$.

If the value of the linear predictor is greater than zero, predict $y=1$, if not, predict $y=0$. 

---

## Another Look at the Swiss Banknotes

.left-code[

```{r swiss1summary, eval=FALSE}
summary(swiss.glm.1)
```

-   For this model, linear predictor is $\beta_0 + \beta_1 \mathsf{margin}$.

-   MLEs are    
$\hat \beta_0 = `r beta0`$ and     
$\hat \beta_1 = `r beta1`$.

-   Hence, the fitted model has linear predictor
    $`r beta0` `r beta1` \times \mathsf{margin}$
]

.right-plot[
```{r, ref.label='swiss1summary', echo=FALSE}
```
]

---

.left-code-wide[
## Another look at the Swiss Banknotes

We classify to class coded by $y=1$ (`genuine` notes) if
    $$\begin{aligned}`r beta0` `r beta1` \times \mathsf{margin} \ge 0 &\Leftrightarrow \mathsf{margin} \le \frac{`r beta0`}{`r -beta1`} \\ &\Leftrightarrow \mathsf{margin} \le `r round(-beta0/beta1,3)`.\end{aligned}$$

This is a pretty sensible place to split, according to the training data.
]

.right-plot-narrow[
```{r, echo=FALSE, fig.dim=c(3.2,4.5), message=FALSE}
swiss.glm.1 |> broom::augment() |>
  mutate(.pred = if_else(.fitted > 0, 'genuine', 'forged'),
         correct = .pred == type) |>
  ggplot() +
  geom_quasirandom(mapping=aes(x=type, y=margin, col=type, alpha=correct)) +
  scale_alpha_manual(values = c(`TRUE` = 1, `FALSE`=0.4)) +
  geom_hline(yintercept = -beta0/beta1) +
  guides(col='none', alpha='none')
```
]

---

## Logistic regression with both predictors

.left-code[

```{r, echo=FALSE}
beta2 = signif(coef(swiss.glm.2), 5)
```

```{r swiss2summary, eval=FALSE}
summary(swiss.glm.2)
```

The classification rule for $y=1$ (`genuine`) for `swiss.glm.2` is:

$$\hat\beta_0 + \hat\beta_1 \times \mathsf{margin} + \hat\beta_2 \times \mathsf{diagonal} > 0$$

where:

 - $\hat\beta_0 = `r beta2[1]`$,
 - $\hat\beta_1 = `r beta2[2]`$,
 - $\hat\beta_2 = `r beta2[3]`$.
]

.right-plot[
```{r, ref.label='swiss2summary', echo=FALSE}
```
]

---

.left-code[
## Logistic regression with both predictors

The classification rule for $y=1$ (`genuine`) for `swiss.glm.2` is:

$$\hat\beta_0 + \hat\beta_1 \times \mathsf{margin} + \hat\beta_2 \times \mathsf{diagonal} > 0$$

where:

 - $\hat\beta_0 = `r beta2[1]`$,
 - $\hat\beta_1 = `r beta2[2]`$,
 - $\hat\beta_2 = `r beta2[3]`$.
]

.right-plot[
```{r swiss2plot, echo=FALSE}
swiss.glm.2 |> broom::augment() |>
  mutate(.pred = if_else(.fitted > 0, 'genuine', 'forged'),
         correct = .pred == type) |>
  ggplot() +
  geom_point(
    mapping=aes(x=margin, y=diagonal,
                col=type, alpha=correct)) +
  scale_alpha_manual(values = c(`TRUE` = 1, `FALSE`=0.4)) +
  guides(alpha='none') +
  geom_abline(intercept=-beta2[1]/beta2[3],
              slope=-beta2[2]/beta2[3])
```
]

---

## Classifying with logistic regression in R

Assume we classify to class coded by $y=1$ if and only if
    $\mathsf{P}(y=1 | \mathbf{x}_0) \ge \tfrac12$.

Recall that this is equivalent to $g(\mathbf{x}_0) = \mathsf{logit}(p(\mathbf{x}_0)) \ge 0$.

One can use `predict()` or `broom::augment()` to get estimated posterior probabilities.

But, beware, prediction for logistic regression can be on one of
    two scales.

-   Predictions on **response scale** are estimated probabilities.

-   Predictions on scale of linear predictor -- i.e.Â logit.

The default in R is prediction on logit scale.

One can specify predictions on response scale through `type` argument to
    `predict()`, or `type.predict` to `augment()`.

-   For example: `augment(my.glm, newdata=test.data, type.predict="response")`.

---

## Probabilities for the Swiss banknotes

.left-code[

```{r swisspred, eval=FALSE}
library(broom)
swiss.glm.2 |>
  augment(newdata = swiss.test) |>
  slice_head(n=3)

swiss.glm.2 |>
  augment(newdata = swiss.test,
      type.predict = 'response') |>
  slice_head(n=3)
```

]

.right-plot[
```{r, ref.label='swisspred', echo=FALSE, message=FALSE}
```

-   For test case 0, $\hat{g(\mathbf{x}_0)} = -18.77$.

-   Hence,
    $\mathsf{P}(y=1 | \mathbf{x}_0) = 1/(1+e^{-\hat g(\mathbf{x}_0)}) = 1/(1+e^{18.77}) \approx 0$.
    
]

---

## Classifying the Swiss banknotes

.left-code[

```{r swisspred2, eval=FALSE}
swiss.glm.pred <-
  swiss.glm.2 |>
  augment(newdata = swiss.test) |>
  mutate(
    .pred_class = factor(
      .fitted > 0,
      levels=c(TRUE, FALSE),
      labels=c('genuine','forged')
      )
  )
swiss.glm.pred
```

We can use the predictions on the logit scale
to classify by creating a factor from
whether it's greater than zero or not.

Need to be careful to get the levels the right
way around.
]

.right-plot[
```{r, ref.label='swisspred2', echo=FALSE, message=FALSE}
```
]

---

## Classifying the Swiss banknotes

The `tidymodels` framework is convenient: there's no worry about the order of the factor, and the code structure works the same for LDA, KDA etc.

```{r eval=TRUE, message=FALSE}
library(parsnip); library(yardstick)

swiss.glm.pred <- logistic_reg() |> fit(type~.,data=swiss.train) |> augment(new_data=swiss.test)

swiss.glm.pred |> slice_head(n=3)
swiss.glm.pred |> conf_mat(truth=type, estimate=.pred_class)
```



---

class: middle, inverse

# A more interesting example

## Symptoms of COVID-19 cases in New Zealand

---

## Symptoms of COVID-19 cases in New Zealand

We have data on both confirmed cases and those tested but confirmed negative, along with a wide variety of symptoms, age, and sex.

These data were analysed in this paper by French et al.:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8556148/

They used decision trees and random forests, among other things, to identify symptoms that are more likely in confirmed cases than those confirmed to be negative.

We read the data into R and process it a little using:

```{r, message=FALSE}
covid_all <- 
  read_csv("https://www.massey.ac.nz/~jcmarsha/data/covid19/symptoms.csv") |>
  mutate( across(everything(), as.factor), 
          Status = fct_relevel(Status, "Not_a_case"))
```

---

## Symptoms of COVID-19 cases in New Zealand

First, we'll split the data into training and testing sets

```{r}
library(rsample)
set.seed(3)
split <- initial_split(covid_all, prop=3/4)

covid.train <- training(split)
covid.test  <- testing(split)
```

Our goal will be to predict the `Status` of an individual, given their symptoms.

---

.left-code[
## Symptoms of COVID-19 cases in New Zealand

```{r symptoms_plot, eval=FALSE}
covid.train |>
  select(-AgeGrp, -Month, -Sex) |>
  pivot_longer(-Status) |>
  group_by(Status, name, value) |>
  summarise(n=n()) |>
  mutate(p=n/sum(n)) |>
  filter(value != "No") |>
  ggplot() +
  aes(y=name,x=p) +
  geom_col() +
  facet_wrap(vars(Status)) +
  labs(x=NULL, y=NULL) +
  scale_x_continuous(
    labels=scales::label_percent()
  )
```
]

.right-plot[
```{r, ref.label='symptoms_plot', echo=FALSE, message=FALSE}
```
]

---

.left-code[
## Symptoms of COVID-19 cases in New Zealand

```{r covid_lr, eval=FALSE}
covid.lr <- logistic_reg() |>
  fit(Status ~ ., data=covid.train)

covid.lr |> tidy() |> print(n=100)
```

This model was fit to all predictor variables: symptoms, age, and sex.

After accounting for all other variables:
- Anosmia (lack of smell) and Ageusia (lack of taste) were important.
- Males were more at risk
- Having a sore throat is a contraindication.

NOTE: This was for the OG Covid. Omicron is also associated with Gastro symptoms.
]

.small-font.right-plot[
```{r, ref.label='covid_lr', echo=FALSE}
```
]

---

## Symptoms of COVID-19 cases in New Zealand

```{r}
covid.pred <- covid.lr |> augment(covid.test)
covid.pred |> conf_mat(truth=Status, estimate=.pred_class)
```

We're getting "Not a case" correct, but are doing very badly at predicting cases!

This may be in part due to the data being **unbalanced**: There are far more 'Not a case' than cases.

While the overall performance is good, if we put this into practice we would be telling over half
of the cases that they're all good. **This is bad in a pandemic!**

---

## The classification threshold

By default, the logistic regression (and most classifiers) classifies a case as being in the second class if $\mathsf{P}(y=1 | \mathbf{x}) > 0.5$.

By using 0.5 as the threshold, we're assuming that each class is equally important.

In a pandemic, it is much more important to identify cases of COVID-19 than it is to tell someone that doesn't have COVID-19 that they don't have it.

We don't really mind if we classify people who aren't cases as being cases - at worst, they'll isolate unnecessarily.

We *do* mind if we classify people with COVID as if they do not, because they won't isolate and may go on to infect others.

Hence, we'd want to use a lower threshold than 0.5, so that we classify more people as cases than as not cases.

---

## ROC curves

By moving the threshold away from 0.5, we're trading off *sensitivity* and *specificity*.

- Sensitivity: correctly identifying true cases 
$\ \ \ \ \ \ \ \ \ \mathsf{P}(\mbox{predicted case} | \mbox{true case})$
- Specificity: correctly identifying negative cases $\ \ \ \mathsf{P}(\mbox{predicted not a case} | \mbox{truly not a case})$

Lowering the threshold means we predict more cases (more $y=1$), increasing sensitivity, but decreasing specificity.

Increasing the threshold means we predict more non-cases (more $y=0$), increasing specificity, but decreasing sensitivity.

The **receiver operating characteristic** (ROC) curve describes how the sensitivity
and specificity change as the threshold changes.

- Plots sensitivity against 1-specificity.
- Use `roc_curve()` in `yardstick()`

---

.left-code[
## ROC curve for COVID-19 Symptoms

```{r roc, eval=FALSE}
covid.roc <-
  covid.pred |>
  roc_curve(Status,
            .pred_Confirmed,
            event_level='second')
ggplot(covid.roc) +
  geom_line(aes(x=1-specificity,
                y=sensitivity)) +
  geom_abline(linetype='dotted') +
  guides(col='none') +
  coord_equal()
```

Choosing a lower threshold will increase
the sensitivity, but decrease specificity;

that is, correctly identify more cases, but misclassify more non-cases.
]

.right-plot[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
covid.roc <-
  covid.pred |>
  roc_curve(Status,
            .pred_Confirmed,
            event_level='second')
ggplot(covid.roc,
       mapping = aes(x=1-specificity, y=sensitivity)) +
  geom_line() +
  geom_abline(linetype='dotted') +
  geom_point(data=covid.roc |> slice_min(abs(.threshold-.5)), size=3) +
  geom_text(data=covid.roc |> slice_min(abs(.threshold-.5)),
            mapping=aes(label=0.5),
            hjust=0, nudge_x=0.03) +
  guides(col='none') +
  coord_equal()
```
]

---

.left-code[
## Using a different threshold

There's no 'correct' threshold, but one simple criteria is to
maximise the sum of the sensitivity and specificities.

```{r}
covid.roc |>
  slice_max(sensitivity+specificity,
            n=1)
```

This results in being as close as possible to the top right of
the plot (100% sensitive and specific).
]

.right-plot[
```{r, echo=FALSE}
ggplot(covid.roc, mapping=aes(x=1-specificity,
                y=sensitivity)) +
  geom_line() +
  geom_point(data=covid.roc |> slice_max(sensitivity+specificity, n=1),
             size=3) +
  geom_text(data=covid.roc |> slice_max(sensitivity+specificity, n=1),
          mapping=aes(label=round(.threshold, 3)),
          hjust=1, nudge_x=-0.03) +
  geom_abline(linetype='dotted') +
  coord_equal()
```
]

---

## Results with the alternative threshold

```{r, warning=FALSE}
covid.pred2 <- covid.pred |>
  mutate(.pred_class_alt = 
           factor(.pred_Confirmed > 0.185,
                  levels = c(FALSE, TRUE),
                  labels = c("Not_a_case", "Confirmed")))
covid.pred2 |>
  conf_mat(truth=Status, estimate=.pred_class_alt)
```

This gives us much better performance on the confirmed cases, but much worse performance overall.

But, the overall misclassification rate is not a good measure of what is important to us.

---

## Assigning misclassification costs

We could instead assign costs to the two errors and compute total costs.

 - assign a cost of 1 to getting not a case wrong.
 - assign a cost of 10 to getting a confirmed case wrong.

```{r}
cost_fun <- function(truth, prediction) {
  case_when(truth == "Not_a_case" & prediction == "Confirmed" ~ 1,
            truth == "Confirmed" & prediction == "Not_a_case" ~ 10,
            TRUE ~ 0)
}

covid.pred2 |> mutate(cost = cost_fun(Status, .pred_class),
                      cost_alt = cost_fun(Status, .pred_class_alt)) |>
  summarise(across(starts_with('cost'), sum))
```

By this measure, the alternative threshold is much better.

---

class: middle, inverse

# More than two classes

## Multinomial Regression

---

## More than two classes

In logistic regression, probability $\mathsf{P}(y = 1|\mathbf{x})$ related to the
    predictors via the linear predictor,
    $g(\mathbf{x}) = \beta_0 + \beta_1 x_{1} + \cdots + \beta_p x_{p}$.

One can extend the model to $C$ classes by defining $C-1$ linear predictors,
    $g_1(\mathbf{x}), \ldots, g_{C-1}(\mathbf{x})$.

The probability of class $j$, $p_j(\mathbf{x}) = \mathsf{P}(j | \mathbf{x})$, is an increasing function of $g_j(\mathbf{x})$.

The probabilities appropriately scaled to all be within interval
$[0,1]$, and that $\sum_{j=1}^{C-1} p_{j}(\mathbf{x}) \le 1$.

The final class probability given by $p_C(\mathbf{x}) = 1 - \sum_{j=1}^{C-1} p_j(\mathbf{x})$      
(that is, 1 - the rest of them). 

This model referred to as a **multinomial** regression because there are multiple classes for the response (target variable).

---

## Implementation of Multinomial Model in R

There are lots of parameters to estimate. There are $C-1$ linear predictors, each with $p+1$ parameters.

Multinomial regression is equivalent to a (classification) neural network with no hidden layers, and a sigmoidal transformation on the output. This is what is done by the `multinom` function in the `nnet` package.

We'll use the `tidymodels` framework via the `multinom_reg()` specification.

As we use `nnet()` under the hood, we need to beware of numerical problems. For instance, it is best to rescale predictors that are large in magnitude.

---


.left-code[
## Computerised wine tasting

We consider some data on Italian wines, with:
- 100 records in the training set and 
- 78 records in an artificial test set.

The aim is to classify the wine `Cultivar` based on $p=13$ chemical measurements.

`Cultivar` has three classes: `A`, `B` and `C`.
]

.right-plot[
```{r, echo=FALSE}
knitr::include_graphics('graphics/chianti.jpg')
```
]

---

## Computerised wine tasting

```{r, message=FALSE}
wine.train <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/wine-train.csv") |>
  mutate(Cultivar = factor(Cultivar))
wine.test <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/wine-test.csv") |>
  mutate(Cultivar = factor(Cultivar))
wine.train
```

---

```{r winemn, include=FALSE, results='hide'}
wine.mn <- multinom_reg() |> fit(Cultivar ~ ., data=wine.train)
```

```{r, echo=FALSE}
get_perf <- function(data, true, pred) {
  data |> summarise(wrong = sum({{pred}} != {{true}}), total = n(),
                      perc = round(wrong/total*100,1))
}
perf.mn <- wine.mn |>
  augment(new_data=wine.test) |>
  get_perf(Cultivar, .pred_class)
```

.left-code[

## Computerised wine tasting

<br/>
<br/>
<br/>
<br/>

Coefficients are shown only for cultivars `B` and `C`,
as the `A` cultivar is determined as probabilities must
sum to 1.
      
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

The misclassification rate is $`r perf.mn$wrong`/`r perf.mn$total` \approx `r perf.mn$perc`\%$.
]

.right-plot[
.small-font[

```{r}
wine.mn <- multinom_reg() |> fit(Cultivar ~ ., data=wine.train)

wine.mn

wine.mn |> 
  augment(new_data=wine.test) |>
  conf_mat(truth=Cultivar,estimate=.pred_class
  )
```
]
]

---

## Computerised wine tasting

We can perform step-wise regression, but not within the `tidymodels` framework, so we've used `multinom`.

```{r, results='hide'}
library(nnet)
wine.mn2 <- multinom(Cultivar ~ ., data=wine.train)
wine.mn2.step <- step(wine.mn2)
wine.mn2.step
```

```{r, echo=FALSE}
wine.mn2.step
```

---

## Computerised wine tasting

Once we know which variables to use from the step wise procedure, we can re-fit using `tidymodels`.

```{r winerefit}
wine.mn2.refit <- multinom_reg() |>
  fit(Cultivar ~ Alcohol + Flav + Hue + Proline, data=wine.train)

wine.mn2.pred <- wine.mn2.refit |>
  augment(new_data = wine.test)

wine.mn2.pred |>
  conf_mat(truth=Cultivar, estimate=.pred_class)
```

```{r, echo=FALSE}
perf.mn2 <- wine.mn2.pred |> get_perf(Cultivar, .pred_class)
```

The misclassification rate of the simpler model is $`r perf.mn2$wrong`/`r perf.mn2$total` \approx `r perf.mn2$perc`\%$, achieved with much fewer variables.

---

# Summary

- Logistic regression is one type of Generalised Linear Model, applicable where the response variable is binary. 

--

- A logistic regression predicts the probability that $y=1$ given predictor variables. 

--

- Logistic regression maps a probability with range $[0,1]$ onto the real line $(-\infty,\infty)$ via the logit ("log-odds") function. 

     - A linear predictor is fit in the logit-transformed-probability space

     - The output from the linear predictor can be transformed back to probabilities using the inverse-logit function.

--

- The model is fit (i.e., parameters are estimated) by maximising the (log) likelihood of the Bernoulli distribution. The Bernoulli produces a $1$ with probability $p$ and a $0$ with probabilty $1-p$.

--

- Costs of the two different types of error (false positive and false negative) can be incorporated into predictions when using it as a classifier. 

--

- Logistic regression can be extended to multinomial regression when the response variable has more than two classes.


