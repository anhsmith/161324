---
title: 'Lecture 9'
subtitle: 'K-nearest neighbours, Trees, Forests and Neural Nets'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(skimr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

swiss.train <- read_csv("../data/swiss-train.csv") %>%
  mutate(type = factor(type))
swiss.test <- read_csv("../data/swiss-test.csv") %>%
  mutate(type = factor(type))
```

## K-Nearest neighbour classification

-   Find $k$ data points in the training set that are most similar to
    the test observation at hand.

-   Assign the test case to the most common class for these proximate
    training data.

-   Similar to missing value imputation.

---

## K-Nearest neighbour classification

There are two main things to consider:

-   Scale. Euclidean distance is often used between observations to
    determine the $k$ most similar items.

-   $k$. Small $k$ will give larger variation as few observations in the
    training data are used to predict. Larger $k$ will give less
    variation based on the majority vote of a large number of
    observations from the training data. However, larger $k$ will have
    bias as we will be comparing the test data to observations that have
    greater disparity in the pattern of predictors.

---

## Breaking ties

There are two ways a tie can occur in $k$-nearest
neighbour classification.

-   Ties between the distances (i.e. more than $k$ observations in the
    training set have the same distance as the nearest $k$ points from
    the test observation. In this case we normally use all observations
    within the same distance as the first $k$. An alternate would be to
    pick the $k$ at random.

-   A tie in the majority vote of the $k$ nearest observations for
    class. In this case we normally assign the class randomly out of the
    top voted classes.

---

## K-Nearest neighbours in R

We will use the `tidymodels` framework, utilising the specification

    nearest_neighbor(mode = 'classification', neighours = 5)

By default this uses the `kknn` package, which automatically scales
the predictor variables to have common variance.

Thus, we need only concern ourselves with determining `k`.

---

## K-Nearest neighbours for the Swiss data

```{r, message=FALSE}
library(parsnip)
library(yardstick)
swiss.knn1 <- nearest_neighbor(mode="classification", neighbors = 1) %>%
  fit(type ~ ., data=swiss.train)
swiss.knn1.pred <- swiss.knn1 %>%
  augment(new_data = swiss.test)
swiss.knn1.pred %>%
  conf_mat(truth=type, estimate=.pred_class)
```

---

## K-Nearest neighbours for the Swiss data

```{r}
swiss.knn2 <- nearest_neighbor(mode="classification", neighbors = 20) %>%
  fit(type ~ ., data=swiss.train)
swiss.knn2.pred <- swiss.knn2 %>%
  augment(new_data = swiss.test)
swiss.knn2.pred %>%
  conf_mat(truth=type, estimate=.pred_class)
```

---

class: middle, inverse

# Classification Trees

---

## Classification Trees

-   Similar to regression trees.

-   Instead of the mean prediction at each node, we use the majority
    vote of class at each node.

-   Instead of minimising RSS to find the next optimal split, we instead
    minimise a measure of node .

---

## Node purity

-   Purity measures how close a node is to being observations of just
    one class.

-   Impurity is then a measure of how close the node is to having
    observations equally distributed among all classes.

-   We can measure impurity using the which measures the chance an
    observation from the node would be misclassified if all the class
    labels were randomly reallocated within that node.

---

## Gini splitting index

Let $\hat{p}_j$ be the proportion of observations
at a node in class $j$, and let $x$ be an observation at that node. If
we randomly reallocate observations to classes within the node,
$$\begin{aligned} P(x \textsf{ misclassified}) &= \sum_{j=1}^C P(x \textsf{ misclassified}|x \in j)P(x \in j)\\ &= \sum_{j=1}^C (1-\hat{p}_j)\hat{p}_j\\
&= 1 - \sum_{j=1}^C \hat{p}_j^2.\end{aligned}$$

We try to find splits that minimise the Gini index.

---

## Gini splitting index

$$P(x \textsf{ misclassified}) = 1 - \sum_{j=1}^C \hat{p}_j^2.$$

-   This is smallest when $\hat{p}_c = 1$ for some $c\in C$ and
    $\hat{p}_j = 0$ for $j \neq c$.

-   This is largest when $\hat{p}_j = \frac{1}{C}$ for all $j$. .

-   To find the best split, work out the Gini index $g$ at the pair of
    nodes $l,r$ formed by each split and then minimise their sum,
    weighted by the proportion $p$ of observations going to each node,
    $$p(l)g(l) + p(r)g(r).$$

---

## Finding the best split

Consider the data

| x | class |
| --: | --- |
| 2.8 |  a  |
| 3.0 |  b  |
| 3.2 |  a  |
| 3.4 |  b  |
| 3.6 |  b  |

-   What possibilities are there for the first split?

-   Find the Gini index of the nodes formed by these splits.

-   What is the best split?

---

.left-code[
## Classification trees in R

```{r}
swiss.rp <- decision_tree(
  mode = 'classification'
  ) %>%
  fit(type ~ ., data=swiss.train)

swiss.rp.pred <- swiss.rp %>%
  augment(swiss.test)

swiss.rp.pred %>%
  conf_mat(truth=type,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
swiss.rp %>%
  extract_fit_engine() %>%
  plot(margin=0.5)
swiss.rp %>%
  extract_fit_engine() %>%
  text()
```
]

---

.left-code[
## Classification tree 2

```{r}
swiss.rp2 <- decision_tree(
  mode='classification',
  min_n=7
  ) %>%
  fit(type ~ ., data=swiss.train)

swiss.rp2.pred <- swiss.rp2 %>%
  augment(swiss.test)

swiss.rp2.pred %>%
  conf_mat(truth=type,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
swiss.rp2 %>% extract_fit_engine() %>% plot(margin=0.1)
swiss.rp2 %>% extract_fit_engine() %>% text()
```
]

---

## Alternate measures of impurity

A number of other measures of impurity
can be used.

-   Cross-entropy information criterion.
    $$I_E = -\sum_{j=1}^C \hat{p}_j\log(\hat{p}_j)$$

-   Misclassification rate. $$M = 1 - \max_j p_j$$

---

## Random forests for classification

- Forests for classification are just collections of classification trees.

- Classification is done by passing each test observation through each tree.

- This gives the most likely class from each tree.

- These are treated as votes for the most likely class for the forest.

    - Can result in ties, which are usually resolved at random.

---

## Random forests for Swiss banknotes

```{r}
set.seed(2000)
swiss.rf <- rand_forest(mode="classification", engine="randomForest") %>%
  fit(type ~ . ,data=swiss.train)
swiss.rf.pred <- swiss.rf %>%
  augment(new_data=swiss.test)
swiss.rf.pred %>%
  conf_mat(truth=type, estimate=.pred_class)
```
  
---

class: middle, inverse

# Neural networks for classification

---

.left-code[
## Neural networks for classification

-   Neural networks attempt to model the brain by assembling a large
    number of interconnected nodes (neurons) which 'fire' or output only
    if the input is large enough.

-   Nodes are ordered into levels and all nodes on one level feed into
    the next level.

    -   Input nodes are the input variables.

    -   These feed into one or more hidden layers or .

    -   The derived features feed into the output node(s).
]

.right-plot[
```{r, echo=FALSE}
source("../common/plots.R")
nnet_plot()
```
]

---

## Neural networks for classification

-   A linear combination of nodes at the previous layer feed through an
    $\phi$ to nodes in the current layer.

-   The default activation function is the sigmoid, also used for
    logistic/multinomial regression
    $$\phi(\theta) = \frac{1}{1+e^{-\theta}}.$$

-   This gives the 'firing' style behaviour as $\theta$ moves through
    zero.

---

## Neural networks for classification

-   We need to define

    -   The number of hidden layers.

    -   The number of derived features in the hidden layers.

    -   The activation functions from one layer to the next.

-   There is little theory to assist us with these, but some simple
    criteria are

    -   Use a single hidden layer.

    -   Use $\lceil p/2\rceil$ derived features, where $p$ is the
        effective number of predictors.

    -   The sigmoid activation function is usually used.

---

## Neural networks for classification

-   The derived features $z_k$ are defined in terms of the predictors
    $x_p$ by
    $$z_k = \phi(\alpha_{0k} + \alpha_{1k}x_1 + \cdots + \alpha_{pk}x_p).$$

-   The outcome nodes for classification are the probability of a class,
    so are also defined using the sigmoid function
    $$\hat{p_c} = \phi(\beta_{0c} + \beta_{1c}z_1 + \cdots + \beta_{kc}z_k).$$

-   The $\alpha$ and $\beta$ terms (the ) are then estimated from
    training data.

---

## Neural networks for classification in R

The actual network used for
classification and the method of fitting the weights $\alpha$ and
$\beta$ terms differs based on the number of classes.

-   With 2 classes, a single output node $\hat{p}$ is used, and the
    cross entropy information criterion is minimised
    $$I_E = \hat{p} \log(\hat{p}).$$

-   With 3 or more classes, an output node per class is used, and
    maximum likelihood is used to estimate the weights.

---

## Fitting neural networks for classification in R

We'll utilise the `mlp()` specification from the `tidymodels` framework:

    mlp(mode='classification', hidden_units=2, penalty=0.1, epochs=1000)

-   The `hidden_units` is the number of hidden nodes.

-   `penalty` is the decay rate for the back propagation algorithm.

-   `epochs` is the number of iterations.

-   Remember that large numeric inputs should be scaled first.

---

## Neural network classification of Swiss data

```{r, message=FALSE}
library(recipes)
swiss.rec <- recipe(type ~ ., data=swiss.train) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep(swiss.train)

set.seed(2001)
swiss.nn <- mlp(mode='classification', hidden_units=2, epochs=500) %>%
  fit(type ~ ., data=bake(swiss.rec, swiss.train))

swiss.nn.pred <- swiss.nn %>% augment(new_data=bake(swiss.rec, swiss.test))

swiss.nn.pred %>% conf_mat(truth=type, estimate=.pred_class)
```

---

## Classification summary

-   Some methods require quantitative predictors:

    -   Linear discriminant analysis, kernel discriminant analysis,
        $k$-nearest neighbours.

-   Some require all predictors on the same scale.

    -   $k$-nearest neighbours. This is handled automatically by `nearest_neighbour` specification.

    -   Neural networks. Scale large predictors first.

-   Some use a linear combination of predictors.

    -   LDA, Logistic or Multinomial regression.

-   Some have strong assumptions.

    -   LDA, Naive Bayes, Logistic or Multinomial regression.

---

## Classification summary

-   Methods with the greatest number of assumptions will do well when
    those assumptions hold.

-   They can do very poorly if the assumptions do not hold.

-   In practice we don't know for certain whether the assumptions hold,
    so many data miners prefer to use more flexible tools such as
    classification trees and neural networks.

-   There are often competitions on the Internet for searching for the
    best classification for a particular problem.

---

## Which classification technique works best?

::: {.center}
![image](syn_class2){width="3.3in"}
:::

    http://www.massey.ac.nz/~jcmarsha/161223/data/syn1-train.csv
    http://www.massey.ac.nz/~jcmarsha/161223/data/syn1-test.csv

---
