---
title: "Cluster Analysis"
subtitle: "161324 Data Mining | Lecture 10"
format: 
  revealjs:
    width: 1050
    height:	700
    transition: fade
    theme: [default, myquarto.scss]
    slide-number: c/t  
    logo: graphics/L_Color.png
    styles:
      - revealjs.dark:
        background-color: #222
        color: #fff
execute:
  echo: true
---


```{r}
#| echo: false

library(kableExtra)
library(tidyverse)
library(ggrepel)
library(animation)
library(workflows)
library(parsnip)
library(tidyclust)
library(recipes)
library(rsample)
library(tune)
library(factoextra)
library(rstatix)
theme_set(theme_bw())
colm <- c("#004B8D", "#E4A024", "#9A3324", "#5E6738", "#D45D00", "#83786F",
          "#A8AD00", "#C5B783", "#4F758B", "#98A4AE", "#0090E9")


```


## Cluster Analysis 

. . .


Recently, we've learned about classification models, which aim to predict the value of an *existing* categorical variable $y$ based on a set of variables $\mathbf{x}$. This is called *supervised* classification.

. . .


Cluster analysis is used to create a *new* categorical variable based on a set of feature variables $\mathbf{x}$. This is called *unsupervised* classification.

. . .


The goal is to classify our objects into groups that have *high within-group similarity* and *low between-group similarity*, with respect to $\mathbf{X}$.

::: footer
Lecture 10 | Cluster Analysis
:::

---

:::: {.columns}

::: {.column width="80%"}

<br><br>
Sometimes there clearly are some groups. <br><br><br>

Sometimes there clearly are no groups. <br><br><br>

Often its something in between.

:::

::: {.column width="20%"}
![](graphics/three_scenarios.png)
:::

::::

<br>

. . .


<br>
*Regardless of the situation, cluster analysis will always produce groups!*


# The $k$-means clustering algorithm {.background-black}

::: footer
$k$-means cluster analysis
:::

## The $k$-means algorithm {.smaller}

::::{.columns}

:::{.column}

<br><br> 

1. Choose number of groups $k$ (here 3).

2. Initialise the $k$ centroids in $\mathbf{X}$ space     
(e.g., choose three points at random).

3. Loop until convergence:
    a. Assign cases to nearest centroid.
    b. Update centroid location.

<br>

It is wise to repeat the whole algorithm many times with different random starts to ensure a good result.
    

:::

:::{.column}

```{r}
#| echo: false
#| fig-show: animate
#| fig-height: 6
#| fig-width: 6

set.seed(2)
km.ani <- kmeans.ani(col = colm[1:3], pch=16)
```

:::

::::

::: footer
$k$-means cluster analysis
:::

## European protein composition {.scrollable}

This dataset contains the proportions (as percentages) of each of nine major sources of protein in the diets of 25 European countries, some time prior to the 1990s.

```{r}
#| echo: true

food <- read_csv("https://massey.ac.nz/~anhsmith/data/food.csv")
kable(food) |> kable_styling(font_size = 18)
```

::: footer
$k$-means cluster analysis
:::

## European protein composition 

```{r}
#| output-location: column
#| fig-width: 5
#| fig-height: 5

food |> ggplot() +
  aes(x=RedMeat,y=WhiteMeat,label=Country) +
  geom_point() + 
  geom_text_repel(size=2.5) +
  xlab("Percent from red meat") +
  ylab("Percent from white meat") +
  theme(aspect.ratio=1, 
        legend.position = "top")
```

::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

First, let's try with 3 clusters and 1 random start.

```{r}
#| output-location: column
set.seed(1)
km_spec_k3_s1 <- k_means(num_clusters = 3) |> 
  parsnip::set_engine("stats", 
                      nstart = 1)

km_fit_k3_s1 <- km_spec_k3_s1 |> 
  fit(~ RedMeat + WhiteMeat, data = food)

km_fit_k3_s1
```
::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

First, let's try with 3 clusters and 1 random start.

```{r}
#| output-location: column
#| fig-width: 5
#| fig-height: 5

km_fit_k3_s1 |> 
  augment(food) |> 
  ggplot() +
  aes(x=RedMeat,
      y=WhiteMeat,
      label=Country, 
      col=.pred_cluster) +
  geom_point() + 
  geom_text_repel(size=2.5) +
  xlab("Percent from red meat") +
  ylab("Percent from white meat") +
  theme(aspect.ratio=1, 
        legend.position = "top")

```

::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

Now, let's do 50 random starts.

```{r}
#| output-location: column
set.seed(1)
km_spec_k3 <- k_means(num_clusters = 3) |> 
  parsnip::set_engine("stats", 
                      nstart = 50)

km_fit_k3 <- km_spec_k3 |> 
  fit(~ RedMeat + WhiteMeat, data = food)

km_fit_k3 
```
::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

Now, let's do 50 random starts.

```{r}
#| output-location: column
#| fig-width: 5
#| fig-height: 5

km_fit_k3 |> 
  augment(food) |> 
  ggplot() +
  aes(x=RedMeat,
      y=WhiteMeat,
      label=Country, 
      col=.pred_cluster) +
  geom_point() + 
  geom_text_repel(size=2.5) +
  xlab("Percent from red meat") +
  ylab("Percent from white meat") +
  theme(aspect.ratio=1, 
        legend.position = "top")

```

::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

What about 2 clusters?

```{r}
#| output-location: column
set.seed(1)
km_spec_k2 <- k_means(num_clusters = 2) |> 
  parsnip::set_engine("stats", 
                      nstart = 50)

km_fit_k2 <- km_spec_k2 |> 
  fit(~ RedMeat + WhiteMeat, data = food)

km_fit_k2
```
::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

What about 2 clusters?

```{r}
#| output-location: column
#| fig-width: 5
#| fig-height: 5

km_fit_k2 |> 
  augment(food) |> 
  ggplot() +
  aes(x=RedMeat,
      y=WhiteMeat,
      label=Country, 
      col=.pred_cluster) +
  geom_point() + 
  geom_text_repel(size=2.5) +
  xlab("Percent from red meat") +
  ylab("Percent from white meat") +
  theme(aspect.ratio=1, 
        legend.position = "top")

```

::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

Or 4?

```{r}
#| output-location: column
set.seed(1)
km_spec_k4 <- k_means(num_clusters = 4) |> 
  parsnip::set_engine("stats", 
                      nstart = 50)



km_fit_k4 <- km_spec_k4 |> 
  fit(~ RedMeat + WhiteMeat, data = food)

km_fit_k4
```
::: footer
$k$-means cluster analysis
:::

## European protein: $k$-means 

Or 4?

```{r}
#| output-location: column
#| fig-width: 5
#| fig-height: 5
 
km_fit_k4 |> 
  augment(food) |> 
  ggplot() +
  aes(x=RedMeat,
      y=WhiteMeat,
      label=Country, 
      col=.pred_cluster) +
  geom_point() + 
  geom_text_repel(size=2.5) +
  xlab("Percent from red meat") +
  ylab("Percent from white meat") +
  theme(aspect.ratio=1, 
        legend.position = "top")

```

::: footer
$k$-means cluster analysis
:::

## How to choose $k$? 
Increasing $k$ will always decrease with within-group error. 

```{r}
#| cache: true
#| layout-nrow: 1
#| fig-width: 3.4
#| fig-height: 3.4
#| echo: false

library(factoextra)

meat <- food |> 
  column_to_rownames(var="Country") |> 
  select(RedMeat,WhiteMeat)

fviz_cluster(
    object = km_fit_k2$fit, 
    data = meat,
    ellipse.type = "none",#euclid",
    star.plot=T,
    repel=T,
    ggtheme=theme_bw()) +
  theme(aspect.ratio=1) +
  ggtitle("k=2") +
  theme(legend.position = "none") +
  xlab("") + ylab("")
  

fviz_cluster(
    object = km_fit_k3$fit, 
    data = meat,
    ellipse.type = "none",#euclid",
    star.plot=T,
    repel=T,
    ggtheme=theme_bw()) +
  theme(aspect.ratio=1) +
  ggtitle("k=3") +
  theme(legend.position = "none") +
  xlab("") + ylab("")


fviz_cluster(
    object = km_fit_k4$fit, 
    data = meat,
    ellipse.type = "none",#euclid",
    star.plot=T,
    repel=T,
    ggtheme=theme_bw()) +
  theme(aspect.ratio=1) +
  ggtitle("k=4") +
  theme(legend.position = "none") +
  xlab("") + ylab("")


fviz_cluster(
    object = kmeans(meat,5), 
    data = meat,
    ellipse.type = "none",#euclid",
    star.plot=T,
    repel=T,
    ggtheme=theme_bw()) +
  theme(aspect.ratio=1) +
  ggtitle("k=5") +
  theme(legend.position = "none") +
  xlab("") + ylab("")
```

```{r}
#| output-location: column
#| fig-width: 6
#| fig-height: 3
library(factoextra)
fviz_nbclust(meat, 
             kmeans, 
             method='wss', 
             k.max = 5)
```

::: footer
$k$-means cluster analysis
:::

## The 'silhouette' method {.smaller}

The silhouette index measures how well the data points fit within their cluster vs a neighbouring cluster.


:::: {.columns}

::: {.column}

For each case $i$, calculate:

$a(i)$ = the average distance from case $i$<br>to all other members of its own cluster.

$b(i)$ = the average distance from case $i$<br>to all members of the nearest neighbouring cluster.

$s(i) = \frac{b(i)-a(i)}{\mathbf{max}(b(i),a(i))}$

:::

::: {.column}

```{r}
#| fig-width: 3.4
#| fig-height: 3.4
#| echo: false
#| fig-show: animate

set.seed(1)

kplot <- food |> 
  add_column(Cluster = as_factor(km_fit_k3$fit$cluster)) |>
  ggplot() +
  aes(x=RedMeat,
      y=WhiteMeat,
      label=Country, 
      col=Cluster) +
  geom_point() + 
  geom_text_repel(size=2.5) +
  xlab("") +
  ylab("") +
  theme(aspect.ratio=1, 
        legend.position = "none")

segs_belg <- food |> 
  transmute(xend=RedMeat,yend=WhiteMeat,Country,
            Cluster=km_fit_k3$fit$cluster) |>
  mutate(x = food |> filter(Country=="Belgium") |> pull(RedMeat),
         y = food |> filter(Country=="Belgium") |> pull(WhiteMeat)) |> 
  filter(Cluster!=3 & Country!="Belgium")
  
set.seed(1)

kplot + 
  geom_segment(
    data=segs_belg |> filter(Cluster==1),
    mapping=aes(x=x,y=y,xend=xend,yend=yend),
    inherit.aes = F
  ) + 
  annotate("text", x=18, y = 14, label="a(i)", vjust=1, hjust=1, size=8)

set.seed(1)

kplot + 
  geom_segment(
    data=segs_belg |> filter(Cluster==2),
    mapping=aes(x=x,y=y,xend=xend,yend=yend),
    inherit.aes = F
  ) + 
  annotate("text", x=18, y = 14, label="b(i)", vjust=1, hjust=1, size=8)

```

:::

::::

The scaling of $s(i)$ by the maximum means that $s(i)$ is always between -1 and 1. 

If $s(i)$ is near 1, the point clearly belongs in its cluster.      
If $s(i)$ is near zero, then the point is "on the fence".       
If $s(i)$ is negative, then the point is more similar to members of another cluster.

::: footer
Silhouette
:::


## The 'silhouette' method

```{r}
#| output-location: column
#| fig-width: 6
#| fig-height: 7

# Create Euclidean distance matrix
dist_meat <- meat |> dist()

# Make silhouette plot
km_fit_k3$fit$cluster |> 
  cluster::silhouette(dist_meat) |> 
  `rownames<-`(rownames(meat)) |> 
  fviz_silhouette(label = T, 
                  print.summary = F) +
  coord_flip()
```


::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 3.4
#| fig-height: 3.4
kplot
```
:::

::: footer
Silhouette
:::

## Using silhouette to choose $k$

For any cluster analysis, we can calculate the *overall average* silhouette score. We can then run the cluster analysis for a range of values of $k$ and choose the value that gives the highest silhouette score. 

The `factoextra` package provides some convenient functions for this.

```{r}
#| output-location: column
#| fig-width: 4
#| fig-height: 3
library(factoextra)
fviz_nbclust(meat, 
             kmeans, 
             method='silhouette', 
             k.max = 6)
```

By this criterion, we'd choose $k$ = 3. 

::: footer
Silhouette
:::

## Leave-one-out silhouette 

```{r}
#| output-location: column
#| cache: true
#| fig-width: 5
#| fig-height: 3


food_cv_metrics <- tune_cluster(
  object = workflow(
    recipe(~ RedMeat + WhiteMeat, 
           data = food), 
    k_means(num_clusters = tune())
    ),
  resamples = vfold_cv(
    food, 
    v = nrow(food)
    ),
  grid = tibble(
    num_clusters=2:6
    ),
  control = control_grid(
    save_pred = TRUE, 
    extract = identity),
  metrics = cluster_metric_set(
    sse_ratio, 
    silhouette_avg
    )
)

food_cv_metrics |> 
  collect_metrics() |> 
  ggplot() +
  aes(x = num_clusters, 
      y = mean, 
      col = .metric) +
  geom_point() + geom_line() +
  ylab("Metric score") + 
  xlab("Number of clusters") 
```

::: footer
Silhouette
:::

## $k$-means for more than two variables  {.smaller}

:::: {.columns}

::: {.column width="35%"}

It's not all about meat! There are actually 9 variables in this dataset.

Note that, although they are all measured as percentages, some vary much more than others. 

It is generally sensible to *normalise* variables (subtract the mean and divide by the standard deviation) before doing $k$-means, or any other analysis that uses Euclidean distances. Otherwise, the variables with larger variances will dominate!

:::

::: {.column width="65%"}
```{r}
#| echo: false
#| fig-width: 7
#| fig-height: 6
#| cache: true

library(GGally) 
food |> select(-Country) |> ggpairs(diag = list(continuous="barDiag"), gaps=0)
```

:::

::::

::: footer
$k$-means cluster analysis
:::

## Choosing $k$ for 9 normalised variables

```{r}
#| output-location: column
#| fig-width: 5
#| fig-height: 4

food_norm <- food |> 
  recipe(~ .) |>
  step_normalize(all_numeric()) |> 
  prep() |> 
  bake(food) |> 
  mutate(Country = food$Country) |> 
  column_to_rownames(var="Country")

food_norm |> 
  fviz_nbclust(kmeans, 
               method='silhouette', 
               k.max = 10)
```

::: footer
$k$-means cluster analysis
:::


## Cluster analysis for nine variables

The `fviz_cluster()` function will now show the clusters on a Principal Components Analysis plot of the nine variables. 

```{r}
km_all_k2 <- kmeans(food_norm, centers=2, nstart=50)
fviz_cluster(km_all_k2, data=food_norm, repel=T, ggtheme=theme_bw())
```

::: footer
$k$-means cluster analysis
:::

## Cluster analysis for nine variables

```{r}
#| fig-width: 8
#| fig-height: 7
#| cache: true

library(GGally) 
food |> 
  select(-Country) |> 
  add_column(Cluster = factor(km_all_k2$cluster)) |> 
  ggpairs(mapping=aes(colour = Cluster))
```

::: footer
$k$-means cluster analysis
:::

## Summary: $k$-means

::: {.increment}

- Inherently based on Euclidean distances.

- It is wise to normalise variables first.

- For large numbers of variables, ordination methods like Principal Components Analysis (PCA) can be used to visualise clusters. 

- Looks for 'spherical clusters'; not so good for irregular shapes. 

- Relatively fast, iterative algorithm.

- The silhouette index can be used to choose $k$.

- One can use actual data points as the cluster centres ('medoids') instead of centroids, giving '$k$-medoid' cluster analysis. This can be implemented with `pam()` in R.

:::

::: footer
$k$-means cluster analysis
:::


# Hierarchical cluster analysis {.background-black}

::: footer
Hierarchical cluster analysis
:::

## Hierarchical cluster analysis {.smaller}

::::{.columns}

:::{.column}

Hierarchical cluster analysis uses a different approach to $k$-means.

:::{.incremental}
- Is deterministic rather than stochastic<br>(no random starts, same every time)
- Hierarchical structure can be shown with a 'dendrogram'
:::

:::


:::{.column}

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 4

fc <- food_norm |> dist() |> hclust()

fd <- fc |> 
  fviz_dend(horiz=T) +
  ylim(food_norm |> dist() |> max(), -3) +
  ylab("Distance") +
  theme(plot.title = element_text(hjust = 0.5))

fd
```

:::

::::

::: footer
Hierarchical cluster analysis
:::

## Hierarchical cluster analysis {.smaller}

::::{.columns}

:::{.column}

Hierarchical cluster analysis uses a different approach to $k$-means.

- Is deterministic rather than stochastic<br>(no random starts, same every time)
- Hierarchical structure can be shown with a 'dendrogram'
- Does not require prior choice of $k$; can 'cut' the dendrogram at any level


:::


:::{.column}

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 4
#| fig-show: animate

fc |> 
    fviz_dend(horiz=T,
              k=2,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=2") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=3,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=3") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=4,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=4") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=5,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=5") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=6,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=6") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=7,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=7") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=8,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=8") +
    theme(plot.title = element_text(hjust = 0.5))

fc |> 
    fviz_dend(horiz=T,
              k=9,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=9") +
    theme(plot.title = element_text(hjust = 0.5))


fc |> 
    fviz_dend(horiz=T,
              k=10,
              k_colors = c("jco"),
              rect = TRUE, 
              rect_border = "jco", 
              rect_fill = TRUE) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    ggtitle("k=10") +
    theme(plot.title = element_text(hjust = 0.5))


# THIS DOESN'T WORK BECAUSE IDFK
# for(i in 2:8) {
#   fc |> 
#     fviz_dend(horiz=T,
#               k=i,
#               k_colors = c("jco"),
#               rect = TRUE, 
#               rect_border = "jco", 
#               rect_fill = TRUE) +
#     ylim(food_norm |> dist() |> max(), -3) +
#     ylab("Distance") +
#     ggtitle(paste0("k=",i)) +
#     theme(plot.title = element_text(hjust = 0.5))
# }

```

:::

::::

::: footer
Hierarchical cluster analysis
:::


## Hierarchical cluster analysis {.smaller}

::::{.columns}

:::{.column}

Hierarchical cluster analysis uses a different approach to $k$-means.

- Is deterministic rather than stochastic<br>(no random starts, same every time)
- Hierarchical structure can be shown with a 'dendrogram'
- Does not require prior choice of $k$; can 'cut' the dendrogram at any level
- Let's look at how it works with a subset of 6 countries and 2 variables

```{r}
# Select 2 vars and 6 countries (in this order)
six_countries <- c("Yugoslavia","Romania","Greece",
                   "Albania","Italy","Bulgaria")

food6 <- food |> 
  select(Country, RedMeat, WhiteMeat) |> 
  slice(Country |> 
          factor(levels = six_countries) |> 
          order(na.last=NA)
        ) |> 
  column_to_rownames("Country")
  
```

:::

:::{.column}

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 4
#| fig-show: animate

fc |> fviz_dend(
  horiz=T,
  k=4,
  k_colors = c("lightgrey","lightgrey","lightgrey","black"),
  # label_cols=c("lightgrey","lightgrey","lightgrey","black"),
  rect = TRUE,
  color_labels_by_k = T,
  rect_border = F,
  rect_fill = F) +
    ylim(food_norm |> dist() |> max(), -3) +
    ylab("Distance") +
    theme(plot.title = element_text(hjust = 0.5))

```

:::

::::

::: footer
Hierarchical cluster analysis
:::

---


```{r}
#| echo: false
#| message: false
dist_type <- "euclidean"
source("hierarchical_demo_lecture10.R")
```

:::: {.columns}

::: {.column}
### Agglomerative hierarchical clustering {.smaller}
::: {style="font-size: 70%;"}
1. Start with each object in its own cluster
2. Repeat until all objects are in one cluster:
    a. Calculate the distance between each <br>pair of objects in the $\mathbf{x}$ space
    b. Join the closest pair of objects
:::
```{r}
#| echo: false
#| fig-height: 3.5
#| fig-width: 3.5
#| fig-align: center

food6_distmat |> 
  as.dist() |> 
  hclust(method = "average") |>
  fviz_dend(horiz=T, show_labels = F, ylab = "", main="") +
  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
  annotate(geom = "segment", 
           x=6.7, xend=6.7,
           y=0, yend=5,
           arrow = arrow(length = unit(0.05, "npc")),
           lwd=2 ) +
  geom_label(aes(x=1:6, y=-.2, label=1:6) ) +
  geom_label(aes(x=c(1.5,3.5,5.5),y=c(1.6,1.5,2.2),
                 label=c("1,2","3,4","5,6"))) +
  geom_label(aes(x=c(4.5,3), y=c(3.2,4.9),
                 label=c("3,4,5,6","1,2,3,4,5,6")),
            ) +
  coord_flip(clip = 'off') +
  theme(plot.margin = margin(t = 0, r = 0, b = 0, l = 30, unit = "pt"))
```

:::

::: {.column}
### Divisive <br>hierarchical clustering 

::: {style="font-size: 70%;"}
1. Start with all objects in one cluster
2. Split into the 'best' 2 groups based on some criterion
3. Continue to split until each object is in its own cluster
:::
```{r}
#| echo: false
#| fig-height: 3.5
#| fig-width: 3.5
#| fig-align: center

food6_distmat |> 
  `dimnames<-`(list(1:nrow(food6_distmat),1:ncol(food6_distmat))) |> 
  as.dist() |> 
  cluster::diana() |>
  fviz_dend(horiz=T, show_labels = T, ylab = "", main="") +
  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank()) +
  annotate(geom = "segment", 
           x=6.7, xend=6.7,
           y=6.7, yend=0,
           arrow = arrow(length = unit(0.05, "npc")),
           lwd=2) +
  geom_label(aes(x=1:6, y=-.2, label=1:6) ) +
  geom_label(aes(x=c(1.5,3.5,5.5),y=c(2.22,1.5,1.6),
                 label=c("1,2","3,4","5,6"))) +
  geom_label(aes(x=c(2.5,4), y=c(4.6,6.7),
                 label=c("1,2,3,4","1,2,3,4,5,6"))) +
  coord_flip(clip = 'off') +
  theme(plot.margin = margin(t = 0, r = 0, b = 0, l = 30, unit = "pt"))
  
```
:::

::::


::: footer
Hierarchical cluster analysis
:::


## Euclidean 'straight line' distances

:::: {.fragment fragment-index=1}
::: {.absolute bottom=520 height="100" left=90}
```{r}
#| echo: false
#| tbl-cap: "Data matrix"

food6 |> kable() |> 
  kable_styling(font_size = 18)
```
:::
::::

::::: {.fragment fragment-index=2}
:::: {.fragment .fade-out fragment-index=3}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat0
```
:::
::::
:::::


:::: {.fragment fragment-index=3}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat_all
```
:::
::::


:::: {.fragment fragment-index=4}
::: {.absolute bottom=150 right=350}
::: {style="font-size: 50%; color: #83786F"}
$$
\begin{align}
\delta_{s,t} &= \sqrt{\sum_{j=1}^p (x_{sj} - x_{tj})^2} \\ \\
\delta_{\text{Yug,Rom}} &= \sqrt{(4.4-6.2)^2 + (5.0-6.3)^2} \\
&=2.22
\end{align}
$$
:::
:::
::::


:::: {.fragment fragment-index=4}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat0 |> kable() |> kable_styling(font_size = 18)

```
:::
::::

::: footer
Euclidean distance
:::

## Hierarchical clustering, 'average' linkage

<!-- BEGINNING -->

::: {.absolute bottom=520 height="100" left=90}
```{r}
#| echo: false
#| tbl-cap: "Data matrix"

food6 |> kable() |> 
  kable_styling(font_size = 18)
```
:::

:::: {.fragment .fade-out fragment-index=11}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat0 |> kable() |> kable_styling(font_size = 18)

```
:::
::::

:::: {.fragment .fade-out fragment-index=12}
::: {.absolute bottom="0%" left="60%"}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Dendrogram"
 
dend0 
```
:::
::::

:::: {.fragment .fade-out fragment-index=13}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat0
```
:::
::::

<!-- STEP 1_bi Join Bul and Ita -->

::::: {.fragment fragment-index=11}
:::: {.fragment .fade-out fragment-index=14}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"

distmat_H |> kable(escape=F) |> kable_styling(font_size = 18)

```
:::
::::
:::::

::::: {.fragment fragment-index=12}
:::: {.fragment .fade-out fragment-index=22}
::: {.absolute bottom="0%" left="60%"}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Dendrogram"
dend_bi 
```
:::
::::
:::::

::::: {.fragment fragment-index=13}
:::: {.fragment .fade-out fragment-index=23}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat_bi
```
:::
::::
:::::

::::: {.fragment fragment-index=14}
:::: {.fragment .fade-out fragment-index=21}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_bi |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::

<!-- STEP 2_bi_ga Join Gre+Alb  -->

::::: {.fragment fragment-index=21}
:::: {.fragment .fade-out fragment-index=24}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_bi_H |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::

::::: {.fragment fragment-index=22}
:::: {.fragment .fade-out fragment-index=32}
::: {.absolute bottom="0%" left="60%"}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Dendrogram"
dend_bi_ga
```
:::
::::
:::::

::::: {.fragment fragment-index=23}
:::: {.fragment .fade-out fragment-index=33}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat_bi_ga
```
:::
::::
:::::

::::: {.fragment fragment-index=24}
:::: {.fragment .fade-out fragment-index=31}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_bi_ga |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::

<!-- STEP 3_bi_ga_ya Join Rom+Yug  -->

::::: {.fragment fragment-index=31}
:::: {.fragment .fade-out fragment-index=34}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_bi_ga_H |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::

::::: {.fragment fragment-index=32}
:::: {.fragment .fade-out fragment-index=42}
::: {.absolute bottom="0%" left="60%"}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Dendrogram"
dend_bi_ga_yr
```
:::
::::
:::::

::::: {.fragment fragment-index=33}
:::: {.fragment .fade-out fragment-index=43}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat_bi_ga_yr
```
:::
::::
:::::

::::: {.fragment fragment-index=34}
:::: {.fragment .fade-out fragment-index=41}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_bi_ga_yr |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::


<!-- STEP 4 _biyr_ga -->

::::: {.fragment fragment-index=41}
:::: {.fragment .fade-out fragment-index=45}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_bi_ga_yr_H |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::

::::: {.fragment fragment-index=42}
:::: {.fragment .fade-out fragment-index=52}
::: {.absolute bottom="0%" left="60%"}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Dendrogram"
dend_biyr_ga
```
:::
::::
:::::

::::: {.fragment fragment-index=43}
:::: {.fragment .fade-out fragment-index=53}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat_biyr_ga
```
:::
::::
:::::

::::: {.fragment fragment-index=45}
:::: {.fragment .fade-out fragment-index=51}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_biyr_ga |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
::::
:::::

<!-- FINAL STEP 5_biyrga -->

::::: {.fragment fragment-index=51}
::: {.absolute bottom="59%" left="60%"}
```{r}
#| echo: false
#| tbl-cap: "Distance matrix"
distmat_biyr_ga_H |> kable(escape=F) |> kable_styling(font_size = 18)
```
:::
:::::

::::: {.fragment fragment-index=52}
::: {.absolute bottom="0%" left="60%"}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Dendrogram"
dend_biyrga
```
:::
:::::

::::: {.fragment fragment-index=53}
::: {.plot-bottom-left}
```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "Scatterplot"
scat_biyrga
```
:::
:::::


::: footer
Hierarchical cluster analysis
:::

## Other group-joining 'linkage' criteria {.smaller}

There are a number of 'linkage' criteria, ways to calculate the distance between groups of objects. 

- 'Average' linkage is simple and generally very sensible (agglomerative, hierarchical cluster analysis with average linkage is sometimes called 'UPGMA'). 
- 'Ward' linkage is another good but more complicated option.
- 'Complete' and 'single' linkage are typically poor options.

::: {.absolute left=100 top=300 width=600}

![](graphics/Linkage_types.png)
:::

::: footer
Hierarchical cluster analysis
:::


## Hierarchical and $k$-means clustering {.smaller}

With $k$-means, the clusters are defined by the distance to a centroid. This results in 'spherical' clusters.

Hierarchical clustering joins the most similar points and groups of points from the 'ground up'. Clusters needn't be any particular shape. It's more about 'gaps'. 

```{r}
#| echo: false
set.seed(7)

dg <- data.frame(
  x1 = c(rnorm(50,-3,1), rnorm(50,3,1)),
  x2 = runif(100,-7,7)
  ) |> 
  mutate(hclust = cbind(x1,x2) |> 
           dist() |> 
           hclust(method="average") |> 
           cutree(2) |> 
           factor(),
         kmeans = cbind(x1,x2) |> 
           dist() |> 
           kmeans(2) |> 
           pluck("cluster") |> 
           factor()
         )
```


::::{.absolute left="0%" width="50%"}
:::{.fragment}
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 5
ggplot(dg,aes(x=x1,y=x2)) + 
  geom_point() + coord_fixed() +
  theme(legend.position = "none") +
  ggtitle("Two clusters?")
```
:::
::::
::::{.absolute left="33%" width="50%"}
::: {.fragment}
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 5
ggplot(dg,aes(x=x1,y=x2,col=hclust)) + 
  geom_point() + coord_fixed() +
  theme(legend.position = "none") +
  ggtitle("Hierarchical average linkage")
```
:::
::::
::::{.absolute left="67%" width="50%"}
::: {.fragment}
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 5
ggplot(dg,aes(x=x1,y=x2,col=kmeans)) + 
  geom_point() + coord_fixed() +
  theme(legend.position = "none") +
  ggtitle("k-means")

```
:::
::::

::: footer
Hierarchical vs k-means
:::

## Hierarchical clustering of all countries {.scrollable}

```{r}
#| echo: false
options(knitr.kable.NA = '.')
```

```{r}
dist(food_norm) |> dist(method = dist_type) |> as.matrix() |> as.data.frame() |> 
  replace_upper_triangle(NA) |>  slice(-1) |> select(-last_col()) |>
  column_to_rownames(var="rowname") |> kable(digits=1) |> kable_styling(font_size = 12)

```



## Hierarchical clustering of all countries

```{r}
library(factoextra)
fviz_nbclust(food_norm, hcut, method = "silhouette")
```

## Hierarchical clustering of all countries

```{r}
hcut2_food <- hcut(food_norm, k = 2) 
fviz_dend(hcut2_food, rect = TRUE, k_colors = c("#004B8D","#E4A024"), main="Dendrogram")
```

## Hierarchical clustering of all countries

```{r}
fviz_cluster(hcut2_food, palette = c("#E4A024","#004B8D"), repel=T, main="Principal Components Plot")
```

## Summary: Hierarchical cluster analysis {.smaller}

::: {.increment}

- Builds a tree-like hierarchical system of groups and subgroups

- Agglomerative hierarchical cluster analysis
      - Begins with each object in its own group
      - Joins the nearest objects step by step until all objects are in one group

- Divisive hierarchical cluster analysis
      - Begins with all objects in one group
      - Splits each group into smaller ones until each object is in its own group

- Can use any dissimilarity measure

- Puts less emphasis on 'spherical' groups than $k$-means

:::

::: footer
Hierarchical cluster analysis
:::


# Clustering binary data {.background-black}

::: footer
Binary data
:::

## Clustering binary data {.scrollable}

Let's look at a dataset of binary characteristics of animals.

```{r}
animals <- cluster::animals |> 
  transmute("warm-blooded"=war-1, "can fly"=fly-1, "vertebrate"=ver-1,
            "endangered"=end-1, "live in groups"=gro-1,"have hair"=hai-1) |>   
  `rownames<-`(c("ant", "bee", "cat", "caterpillar", "chimpanzee", "cow", "duck", 
                 "eagle", "elephant", "fly", "frog", "herring", "lion", "lizard", 
                 "lobster", "human", "rabbit", "salmon", "spider", "whale"))

# Replace some NAs and fix some errors -- humans are not endangered!
animals[c('frog','lobster','salmon'),'live in groups'] <- 1
animals[c('lion','human','spider'),'endangered'] <- c(1,0,0)

animals
```

::: footer
Binary data
:::

## Calculating dissimilarity with binary data 

::: {style="font-size: 70%;"}

Remember, distance or dissimilarity is calculated between each pair of objects. 

The two methods considered here for binary data, 'simple matching' and 'Jaccard', differ in one important aspect: the treatment of 'double zeros'.
<br>
:::

::::{.columns}
:::{.column}
### Simple matching
::: {style="font-size: 70%;"}
What proportion of variables do the objects have different values?
:::
```{r}
animals[c('bee','cat'),] |> 
  t() |> 
  data.frame() |> 
  mutate(`Same?` = bee==cat)
```
::: {style="font-size: 70%;"}
Dissimilarity =      
      4 different / 6 variables = 0.67
:::
:::
:::{.column}
### Jaccard
::: {style="font-size: 70%;"}
What proportion of variables have different values, excluding double zeros?
:::
```{r}
#| warning: false
animals[c('bee','cat'),] |> 
  t() |> data.frame() |> 
  group_by(bee, cat) |> 
  summarise(n = n())
```
::: {style="font-size: 70%;"}
Dissimilarity =       
      4 different / 5 non-double-zeros = 0.8
:::
:::
::::


::: footer
Binary data
:::

---

::::{.columns}
:::{.column}
### Simple matching
::: {style="font-size: 70%;"}
Manhattan distance is simply the sum of absolute differences. For binary data, the '`manhattan`' method is equivalent to simple matching without dividing by the number of variables.
:::
```{r}
#| fig-height: 4
#| fig-width: 4
dist(animals, method='manhattan') |> hclust() |>
 fviz_dend(horiz = T, main = "") + ylim(6,-1.5)
```
:::

:::{.column}
### Jaccard
::: {style="font-size: 70%;"}
The '`binary`' method in the `dist()` function is equivalent to Jaccard dissimilarity.
<br>
<br>
<br>
:::
```{r}
#| fig-height: 4
#| fig-width: 4
dist(animals, method='binary') |> hclust() |> 
 fviz_dend(horiz = T, main = "") + ylim(1,-.4)
```

:::
::::


::: footer
Binary data
:::



# Summary {.background-black}

::: footer
Hierarchical cluster analysis
:::

## Summary {.smaller}

:::{.incremental}

- Cluster analysis is unsupervised classification. There is no target variable. 

- The goal of cluster analysis is to create a new system of groups that have low within-group dissimilarity and high between-group dissimilarity, with respect to our features.

- There are many methods of cluster analysis. We have focused on:
     - $k$-means
     - agglomerative hierarchical clustering
     
     Other methods include $k$-medoids and divisive hierarchical clustering.

- Euclidean distance, or 'straight-line' distance, is the most common measure of dissimilarity for numerical variables, but there are other options.

- Different types of data require different measures. For example, 'simple matching' or 'Jaccard' can be used for binary variables. Different measures often yield different results.

- Metrics such as 'silhouette' can help decide how many groups to make. 

:::

