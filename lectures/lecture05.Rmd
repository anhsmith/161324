---
title: 'Lecture 5'
subtitle: 'Trees and Forests'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

wage.train <- read_csv("../data/wage-train.csv")
wage.test  <- read_csv("../data/wage-test.csv")
```

## Regression Trees

-   Linear regression models make strong assumptions about the
    functional form of the relationship between target and predictor
    variables.

-   Such models work very well when these assumptions are valid, and can
    fail spectacularly when they are not.

-   In this lecture we introduce regression trees, which provide a more
    flexible approach to prediction.

-   Regression trees are sometimes referred to by alternative names:

    -   CART (classification and regression trees);

    -   Recursive partitioning methods.

---

## Overview of Regression Trees

-   Basic idea is to split the data into groups using predictors, and
    then estimate the response within each group by a fixed value.

-   Most commonly, the groups are formed by a sequence of **binary splits**.

-   Resulting partition of the data can be described by a **binary tree**.

-   Perhaps easiest to understand through an example.

---

.left-code[## Regression Tree for a Toy Dataset

-   Artificial dataset of 400 observations with a single predictor $x$
    and a target $y$.

-   Relationship between $x$ and $y$ is quite complicated -- difficult
    to model using linear regression.
]

.right-plot[
```{r toytree1, echo=FALSE}
set.seed(2012)
x <- runif(400,0,10)
x <- sort(x)
y <- 1+ sin(1.5*x/(1+x^2/100)) + rnorm(400,sd=0.25)
toytree <- data.frame(x=x, y=y)
ggplot(toytree) +
  geom_point(mapping = aes(x=x, y=y))
```
]

---

.left-code-wide[## Example continued

```{r, echo=FALSE}
library(rpart)
toy.rp <- rpart(y ~ x, cp=0.1, data=toytree)
toy_splits <- round(toy.rp$splits[,"index"],3)
```

-   Tree formed by recursive binary partitioning of data.

-   All data starts at 'root' node (at top).

-   First split (branching) of the tree is at its root.

    -   Data with $x < `r toy_splits[1]`$ branch left.

    -   Data with $x \ge `r toy_splits[1]`$ branch right.

-   Second split is to further divide the $x < `r toy_splits[1]`$ group to form two
    sub-groups.

    -   Data with $x \ge `r toy_splits[2]`$ branch left.

    -   Data with $x < `r toy_splits[2]`$ branch right.
]

.right-plot-narrow[
```{r, echo=FALSE, fig.dim=c(3.2,4.5)}
plot(toy.rp,compress=TRUE,margin=0.15)
text(toy.rp,cex=1)
```
]

---

.left-code[## Example continued

-   In principle we could partition the data further.

-   For simplicity we will stop with the current set of **leaves**.

-   At each leaf the target is estimated by the mean value of the
    y-variable for all data at that leaf.

-   This means that the predictions are constant across the range of
    x-values defining each leaf.
]

.right-plot[
```{r, echo=FALSE}
rect1 <- data.frame(x=c(-1,-1,toy_splits[2],toy_splits[2]), y=c(-0.7,2.9,2.9,-0.7), col=grey(0.95))
rect2 <- data.frame(x=c(toy_splits[2],toy_splits[2],toy_splits[1],toy_splits[1]), y=c(-0.7,2.9,2.9,-0.7), col=grey(0.85))
rect3 <- data.frame(x=c(toy_splits[1],toy_splits[1],11,11),y=c(-0.7,2.9,2.9,-0.7), col=grey(0.95))
rects <- bind_rows(lst(rect1, rect2, rect3), .id="rect")
segs <- bind_rows(lst(seg1 = data.frame(x = c(toy_splits[1],11), y = 1.794),
                      seg2 = data.frame(x = c(toy_splits[2], toy_splits[1]), y = 0.5704),
                      seg3 = data.frame(x = c(-1, toy_splits[2]), y = 1.645)), .id="seg")

ggplot(toytree) +
  geom_polygon(data = rects, mapping = aes(x=x, y=y, fill=col)) +
  scale_fill_identity() +
  geom_point(mapping = aes(x=x, y=y), col="#DF536B") +
  geom_line(data=segs, mapping = aes(x=x, y=y, group=seg), size=1) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
```
]

---

## The Anatomy of a Tree

-   In mathematical terms, a tree is a type of *undirected graph*.

-   In the language of graph theory:

    -   the branches of the trees are called *edges* or *arcs*.

    -   the vertices are often referred to as *nodes*.

-   Regression trees are *rooted* (with the root of the tree
    conventionally plotted at the top of the tree)

-   Vertices at the tips of the tree (i.e. those without further splits)
    are called *leaves*.

---

## How to Grow a Tree

-   At each stage in tree growth, want to select the best split.

    -   Which node to split at?

    -   Which variable to split with?

    -   What value of that variable to split at?

-   Best split usually regarded as the one which results in smallest
    possible *residual sum of squares* on the training data:
    $$\mathsf{RSS}_{train} = \sum_{i=1}^n \left ( y_i - \hat y_i \right )^2$$
    where $\hat y_i$ is the tree-based prediction.

---

## How to Grow a Tree

-   Optimal (to minimize RSS) to define $\hat y_i$ to be mean
    value at leaf containing record $i$.

    -   I.e. if $i$ is assigned to leaf $\ell$, then
        $\hat y_{i} = \bar y_{\ell}$ where $\bar y_{\ell}$ is the mean
        of all data at that leaf.

-   The residual sum of squares for a tree can be written in the form
    $$\mathsf{RSS} = \sum_{\ell} \sum_{i \in C_{\ell}} \left ( y_{i} - \bar y_{\ell} \right )^2$$
    where the set $C_{\ell}$ indexes the observation at leaf $\ell$.

---

## A Very Simple Choice of Splits

-   Data:

| $x$ | $y$ |
| --: | --: |
| 0.1 |  2  |
| 0.3 |  3  |
| 0.5 |  7  |

-   It is clear that we need consider only two splits:

    (i) split the data according to whether $x < 0.2$ or $x \ge 0.2$; or

    (ii) split the data according to whether $x < 0.4$ or $x \ge 0.4$.

---

## Example continued

-   For tree (i):

    -   Mean for left-hand leaf is $\bar{y}_1 = 2$

    -   Mean for right-hand leaf is $\bar{y}_2 = (3+7)/2 = 5$.

    $$\mathsf{RSS} = \left [ (2-2)^2 \right ] + \left [ (3-5)^2 + (7-5)^2 \right ] = 8.$$

-   For tree (ii):

    -   Mean for left-hand leaf is $\bar{y}_1 = (2+3)/2 = 2.5$

    -   Mean for right-hand leaf is $\bar{y}_2 = 7$.

    $$\mathsf{RSS} = \left [ (2-2.5)^2 + (3-2.5)^2 \right ] + \left [ (7-7)^2 \right ] = 0.5.$$

-   We conclude that tree (ii) is preferable.

-   Could have got same results using different splits;

    -   E.g. a split of $x < 0.11$ and $x \ge 0.11$ for tree (i).

-   Conventional to split at a midpoint.

---

## Categorical Predictor Variables

-   So far we have dealt only with splits based on numerical predictors.

-   With factors split by dividing the factor levels into two sets.

-   Observations sent down left or right branch according to this
    division.

-   The separation of the levels is again done so as to minimize the
    residual sum of squares.

---

## Implementing Regression Trees in R

-   Regression trees constructed in R using `rpart`

    -   Name is an abbreviation of *recursive partitioning*.

-   Function is part of the `rpart` library, which must be pre-loaded.

-   Basic syntax for `rpart` mirrors that for `lm`.

-   For example:

        rpart(y ~ x1 + x2, data=mydata)

    fits regression tree with target variable `y` and predictors `x1`
    and `x2`, from data frame `mydata`.

---

## Regression Tree for the Wage Data

```{r}
library(rpart)
wage.rp <- rpart(WAGE ~ . ,data=wage.train)
summary(wage.rp)
```

---

## Example continuedR code continued

```{r}
plot(wage.rp, compress=TRUE)
text(wage.rp)
```

---

## Example continuedCommentary on R code

-   We start by loading the `rpart` package.

-   The second command saves a regression tree as `wage.rp`.

    -   Note the use of the dot in the model formula (works as for
        `lm`).

-   Summary of `wage.rp` describes formation of all splits.

-   The table provides information on the extent to which each new split
    has improved the model.

-   The remainder of the summary output describes each node in turn.

    -   We get an ordered list of the top few possible splits (the
        *primary splits*).

    -   Listed improvements are measured in terms of a the fractional
        reduction in the residual sum of squares for the data at the
        node in question (the *parent* node for the split).

---

## Example continuedCommentary on R code continued

-   First node (at root) formed by splitting on the variable `OCCUP`.

    -   Observations at levels 1, 3, 5 and 6 (i.e. `Management`,
        `Clerical`, `Professional` and `Other`) sent down the left-hand
        branch

    -   Observations at levels 2 and 4 (i.e. `Sales` and `Service`) sent
        down the right-hand branch.

-   Splitting in this way results in a 17.2% reduction in the residual
    sum of squares.

-   Next best option would have been to split according to `EDU < 13.5`,
    which would have resulted in a 14% reduction in residual sum of
    squares.

-   Leaves of tree (i.e. terminal nodes) are described by the number of
    observations, the mean response (i.e. prediction) and sum of squares
    at each one.

    -   For example, there is a leaf (node 30) with 11 observations and
        mean target value $12.18$.

---

## Example continuedComments on plot

-   `plot(wage.rp)` plots just the structure of the regression tree.

    -   Optional argument `compress=TRUE` tends to improve appearance of
        plot.

-   We add text describing the splits by `text(wage.rp`).

-   Note that the splits based on factors use alphabetically ordered
    letters to represent the factor levels (i.e. 'a' for level 1, 'b'
    for level 2 and so forth).

---

## Missing Data and Surrogate Splits

-   Regression trees can handle missing data using *surrogate splits*.

-   A surrogate split is an alternative that approximates the (best)
    primary split.

-   If an observation lacks data on the current primary split variable,
    then it will be assigned to a branch by applying the surrogate
    splitting rule.

---

## Example revisited

    Node number 1: 400 observations,    complexity param=0.1718578
      mean=8.930075, MSE=24.86239 
      left son=2 (280 obs) right son=3 (120 obs)
      Primary splits:
          OCCUP splits as  LRLRLL,   improve=0.17185780, (0 missing)
          EDU   < 13.5 to the left,  improve=0.13989640, (0 missing)
          AGE   < 26.5 to the left,  improve=0.06522658, (0 missing)
          SEX   splits as  LR,       improve=0.03670674, (0 missing)
          EXP   < 9.5  to the left,  improve=0.03420540, (0 missing)
      Surrogate splits:
          EDU < 14.5 to the left,  agree=0.832, adj=0.442, (0 split)
    .....

-   Surrogate split on first note is $\verb"EDU" < 14.5$.

-   Any record with missing value on `OCCUP` will follow split according
    to this surrogate.

---

## Prediction Using Regression Trees

-   Prediction works by running each test case down the tree.

    -   Uses surrogate splits where necessary.

-   Use R function `predict`.

-   Syntax mirrors prediction for linear models.

---
## Tree-based Prediction for the Wage Data

```{r}
wage.rp.pred <- predict(wage.rp,wage.test)
head(wage.rp.pred)
mse.rp <- mean((wage.rp.pred - wage.test$WAGE)^2)
mse.rp
```

-   MSE of prediction for regression tree is $24.4$.

-   MSE of prediction with linear model was $21.9$.

-   Suggests that relationship between target and predictors is
    reasonably linear.

---

## Pruning Trees

-   In principle we can continue to split nodes of a tree until there is
    one leaf corresponding to each unique observed set of predictor
    variables.

    -   If predictor sets are different for all records, then this
        implies a single data point at each leaf.

-   This would lead to a highly unstable regression tree.

-   Consider bias-variance trade off:

    -   One observation per leaf implies lots of flexibility in model
        (so low bias) but high variability.

    -   Many observations per leaf reduce flexibility (introduce bias)
        but reduce variability.

---

.left-code[
## Return of the Toy Dataset

-   Optimal prediction will aim for 'predictable' trend.

-   No hope of predicting 'noise' based on available predictor.
]

.right-plot[
```{r, echo=FALSE}
ggplot(toytree) +
  geom_point(mapping = aes(x=x, y=y))
```
]
---

.left-code[## Example continued

-   Recall that regression trees imply 'step function' estimation of trend.

-   The more complex the tree, the more complex the step function.
]

.right-plot[
```{r, echo=FALSE}
toy.rp.1 <- rpart(y ~ x, cp=0.1, data=toytree)
toy.rp.2 <- rpart(y ~ x, cp=0.01, data=toytree)
toy.rp.3 <- rpart(y ~ x, cp=0.000001, data=toytree, minsplit=3)
par(mar=c(4,1,1,1))
par(mfrow=c(3,2))
plot(toy.rp.1,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.1),lwd=2)
plot(toy.rp.2,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.2),lwd=2)
plot(toy.rp.3,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.3),lwd=2)
```
]

---

## Model Complexity

-   Can think about bias-variance trade off in terms of model
    complexity.

-   Low complexity -- high bias -- low variability.

-   High complexity -- low bias -- high variability.

-   Motivates introduction of *complexity parameter* $cp$.

-   $cp$ specifies minimum improvement in model fit that warrants
    inclusion of a new split in tree.

-   Implies tree growth ceases when no split produces an improvement of
    at least $cp$.

-   $cp$ relates to relative improvement in residual sum of squares.

-   Specified by `cp` argument of `rpart`, default `cp`$=0.01$.

---

.left-code[## Regression Trees of different complexities

```{r}
wage.rp.1 <- rpart(WAGE ~ .,
                   data=wage.train,
                   cp=0.1)
wage.rp.2 <- rpart(WAGE ~ .,
                   data=wage.train,
                   cp=0.001)
```
]

.right-plot[
```{r, echo=FALSE}
par(mfrow=c(2,1), mar=c(0,0,1,0))
plot(wage.rp.1,compress=T,margin=0.1)
text(wage.rp.1,cex=0.7)
title("cp=0.1")
plot(wage.rp.2,compress=T,margin=0.1)
text(wage.rp.2,cex=0.7)
title("cp=0.001")
```
]

---

## MSE of Prediction for Models

```{r}
wage.rp.pred <- wage.test %>%
  bind_cols(
    .pred1 = predict(wage.rp.1, newdata=wage.test),
    .pred2 = predict(wage.rp.2, newdata=wage.test)
  )
wage.rp.pred %>%
  summarise(MSE.1 = mean((.pred1 - WAGE)^2),
            MSE.2 = mean((.pred2 - WAGE)^2))
```

-   Prediction using `cp`$=0.001$ better than default value in this
    case.

---

## Validation

-   Default value of $cp=0.01$ is only a rule-of-thumb.

-   Can often get better predictions by adjusting this parameter.

-   No use tuning on training data -- will always select most complex
    tree (overfitting).

-   One approach is to divide available data into training and
    *validation* datasets.

    -   Usually training set larger -- e.g. training to validation size
        ratio of 3:1, or even 9:1.

-   Training data used for growing tree.

-   Predictive accuracy assessed by predicting on validation dataset.

-   Pick value of $cp$ to give minimize prediction error.

---

## Cross-Validation

-   Validation results depend on particular decomposition into training
    and validation subsets.

-   Cross-validation addresses this issue.

-   The idea is that we split the data into equally size blocks
    (subgroups).

-   Each block in turn being set aside as the validation data, with
    remaining blocks combining to form training set.

-   Prediction error averaged over results for each validation set.

-   Common to use *10-fold* cross-validation when data split into 10
    blocks (so 90% to 10% training to validation ratio at each stage).

    -   Extreme version is 'leave-one-out' cross-validation.

-   Can be run for different values of $cp$.

---

.left-code[## Cross-Validation for the Wage Data Trees

-   A table of model characteristics for different values of $cp$ can be
    obtained using the command `printcp`.

-   Mirrors output from `summary` of a regression tree.

```{r wagecp, eval=FALSE}
printcp(wage.rp.2)
```
]

.small-font.right-plot[
```{r, ref.label='wagecp', echo=FALSE}
```
]

---

## Example continued

-   `xerror` column contains cross-validation estimates of the
    (relative) prediction error.

-   `xstd` is the standard error (i.e. an estimate of the uncertainty)
    for these cross-validation estimates.

-   Minimum cross-validation error occurs when $cp=0.02396$.

-   Note that differences between the values of `xerror` are quite small
    compared to the entries in `xstd`, indicating that the
    cross-validation results do not provide a strong preference for any
    tree with between 1 and 31 vertices.

---

class: middle,inverse

# Forests

---
