---
title: 'Lecture 5'
subtitle: 'Trees and Forests'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

wage.train <- read_csv("../data/wage-train.csv")
wage.test  <- read_csv("../data/wage-test.csv")
```

## Regression Trees

-   Linear regression models make strong assumptions about the
    functional form of the relationship between target and predictor
    variables.

-   Such models work very well when these assumptions are valid, and can
    fail spectacularly when they are not.

-   In this lecture we introduce regression trees, which provide a more
    flexible approach to prediction.

-   Regression trees are sometimes referred to by alternative names:

    -   CART (classification and regression trees);

    -   Recursive partitioning methods.

---

## Overview of Regression Trees

-   Basic idea is to split the data into groups using predictors, and
    then estimate the response within each group by a fixed value.

-   Most commonly, the groups are formed by a sequence of **binary splits**.

-   Resulting partition of the data can be described by a **binary tree**.

-   Perhaps easiest to understand through an example.

---

.left-code[## Regression Tree for a Toy Dataset

-   Artificial dataset of 400 observations with a single predictor $x$
    and a target $y$.

-   Relationship between $x$ and $y$ is quite complicated -- difficult
    to model using linear regression.
]

.right-plot[
```{r toytree1, echo=FALSE}
set.seed(2012)
x <- runif(400,0,10)
x <- sort(x)
y <- 1+ sin(1.5*x/(1+x^2/100)) + rnorm(400,sd=0.25)
toytree <- data.frame(x=x, y=y)
ggplot(toytree) +
  geom_point(mapping = aes(x=x, y=y))
```
]

---

.left-code-wide[## Example continued

```{r, echo=FALSE}
library(rpart)
toy.rp <- rpart(y ~ x, cp=0.1, data=toytree)
toy_splits <- round(toy.rp$splits[,"index"],3)
```

-   Tree formed by recursive binary partitioning of data.

-   All data starts at 'root' node (at top).

-   First split (branching) of the tree is at its root.

    -   Data with $x < `r toy_splits[1]`$ branch left.

    -   Data with $x \ge `r toy_splits[1]`$ branch right.

-   Second split is to further divide the $x < `r toy_splits[1]`$ group to form two
    sub-groups.

    -   Data with $x \ge `r toy_splits[2]`$ branch left.

    -   Data with $x < `r toy_splits[2]`$ branch right.
]

.right-plot-narrow[
```{r, echo=FALSE, fig.dim=c(3.2,4.5)}
plot(toy.rp,compress=TRUE,margin=0.15)
text(toy.rp,cex=1)
```
]

---

.left-code[## Example continued

-   In principle we could partition the data further.

-   For simplicity we will stop with the current set of **leaves**.

-   At each leaf the target is estimated by the mean value of the
    y-variable for all data at that leaf.

-   This means that the predictions are constant across the range of
    x-values defining each leaf.
]

.right-plot[
```{r, echo=FALSE}
rect1 <- data.frame(x=c(-1,-1,toy_splits[2],toy_splits[2]), y=c(-0.7,2.9,2.9,-0.7), col=grey(0.95))
rect2 <- data.frame(x=c(toy_splits[2],toy_splits[2],toy_splits[1],toy_splits[1]), y=c(-0.7,2.9,2.9,-0.7), col=grey(0.85))
rect3 <- data.frame(x=c(toy_splits[1],toy_splits[1],11,11),y=c(-0.7,2.9,2.9,-0.7), col=grey(0.95))
rects <- bind_rows(lst(rect1, rect2, rect3), .id="rect")
segs <- bind_rows(lst(seg1 = data.frame(x = c(toy_splits[1],11), y = 1.794),
                      seg2 = data.frame(x = c(toy_splits[2], toy_splits[1]), y = 0.5704),
                      seg3 = data.frame(x = c(-1, toy_splits[2]), y = 1.645)), .id="seg")

ggplot(toytree) +
  geom_polygon(data = rects, mapping = aes(x=x, y=y, fill=col)) +
  scale_fill_identity() +
  geom_point(mapping = aes(x=x, y=y), col="#DF536B") +
  geom_line(data=segs, mapping = aes(x=x, y=y, group=seg), size=1) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
```
]

---

## How to Grow a Tree

-   At each stage in tree growth, want to select the best split.

    -   Which node to split at?

    -   Which variable to split with?

    -   What value of that variable to split at?

-   Best split usually regarded as the one which results in smallest
    possible *residual sum of squares* on the training data:
    $$\mathsf{RSS}_{train} = \sum_{i=1}^n \left ( y_i - \hat y_i \right )^2$$

-   Optimal (to minimize RSS) to define $\hat y_i$ to be the mean
    value at leaf containing record $i$. All predictions at leaf $\ell$
    are assigned the value $\bar y_{\ell}$.

-   The residual sum of squares for a tree can be written in the form
    $$\mathsf{RSS} = \sum_{\ell} \sum_{i \in C_{\ell}} \left ( y_{i} - \bar y_{\ell} \right )^2$$
    where the set $C_{\ell}$ indexes the observation at leaf $\ell$.

---

## A Simple Choice of Splits
.left-code[
-   Suppose we have just 3 observations:

| $x$ | $y$ |
| --: | --: |
| 0.1 |  2  |
| 0.3 |  3  |
| 0.5 |  7  |

-   It is clear that we need consider only two splits:

    (i) split the data according to whether $x < 0.2$ or $x \ge 0.2$; or

    (ii) split the data according to whether $x < 0.4$ or $x \ge 0.4$.
]
--
.right-plot[
For tree (i):

-   Mean for left-hand leaf is $\bar{y}_1 = 2$

-   Mean for right-hand leaf is $\bar{y}_2 = (3+7)/2 = 5$.

$$\mathsf{RSS} = \left [ (2-2)^2 \right ] + \left [ (3-5)^2 + (7-5)^2 \right ] = 8.$$
]

--
.right-plot[
For tree (ii):

-   Mean for left-hand leaf is $\bar{y}_1 = (2+3)/2 = 2.5$

-   Mean for right-hand leaf is $\bar{y}_2 = 7$.

$$\mathsf{RSS} = \left [ (2-2.5)^2 + (3-2.5)^2 \right ] + \left [ (7-7)^2 \right ] = 0.5.$$
]

--
.right-plot[
**We conclude that tree (ii) is preferable.**
]

---

## Categorical Predictor Variables

-   So far we have dealt only with splits based on numerical predictors.

-   Factors are split by dividing the factor levels into two sets.

-   Observations are then sent down left or right branch according to this
    division.

-   The separation of the levels is again done so as to minimize the
    residual sum of squares.

-   Note that with factors with many levels, there are a (very) large number
    of possible splits to consider.

---

.left-code[
## Regression Tree for the Wage Data

```{r wagerp, eval=FALSE}
library(rpart)
wage.rp <- rpart(WAGE ~ . ,
                 data = wage.train)
summary(wage.rp)
```

The `rpart()` command fits a regression tree,
using a similar syntax to `lm()`.

Note copious output from `summary()`.

We see the first split is on `OCCUP` where
the first, third, fifth and sixth levels
go down the left branch, while the second
and fourth levels go to the right.

This split was chosen as it improved the RSS by 17.2%.
The next best option is to split on `EDU < 13.5`.
]

.scroll-box-right.small-font.right-plot[
```{r, ref.label="wagerp", echo=FALSE}
```
]

---

.left-code[
## Regression Tree for the Wage Data

```{r wagerp2, eval=FALSE}
plot(wage.rp,
     compress = TRUE,
     margin = 0.1)
text(wage.rp)
```

The `plot` command visualises the tree,
whilst the `text` command adorns it with
labels.

The `compress=TRUE` tends to make the tree
more visually appealing, while `margin=0.1`
adds a bit of whitespace.

Note that factor levels are reassigned the values
a, b, c etc.
]

.right-plot[
```{r, echo=FALSE}
par(mar=c(0,0,0,0))
plot(wage.rp,
     compress = TRUE,
     margin = 0.1)
text(wage.rp)
```
]

---

.left-code[
## Missing Data and Surrogate Splits

-   Regression trees can handle missing data using *surrogate splits*.

-   A surrogate split is an alternative that approximates the (best)
    primary split.

-   Surrogate split on first node is $\verb"EDU" < 14.5$.

-   Any record with missing value on `OCCUP` will follow split according
    to this surrogate.
]

.scroll-box-right.small-font.right-plot[
```{r, ref.label="wagerp", echo=FALSE}
```
]
---

## Prediction Using Regression Trees

Prediction works by running each test case down the tree, using surrogate splits where necessary.

```{r}
wage.pred.rp <- 
  wage.test %>%
  bind_cols(
    .pred = predict(wage.rp, newdata=wage.test)
  )
wage.pred.rp %>%
  summarise(MSE =  mean((.pred - WAGE)^2))
```

-   MSE of prediction for regression tree is $24.4$.

-   MSE of prediction with linear model was $21.9$.

-   Suggests that relationship between target and predictors is
    reasonably linear.

---

## Pruning Trees

-   In principle we can continue to split the nodes of a tree until there is
    one leaf corresponding to each unique observed set of predictor
    variables.

    -   If predictor sets are different for all records, then this
        implies a single data point at each leaf.

-   This would lead to a highly unstable regression tree.

-   Consider the bias-variance trade off:

    -   One observation per leaf implies lots of flexibility in model
        (so low bias) but high variability.

    -   Many observations per leaf reduce flexibility (introduce bias)
        but reduce variability.

---

.left-code[
## Return of the Toy Dataset

-   Optimal prediction will aim for 'predictable' trend.

-   We have no hope of predicting the 'noise' based on the single available predictor.
]

.right-plot[
```{r, echo=FALSE}
ggplot(toytree) +
  geom_point(mapping = aes(x=x, y=y))
```
]
---

.left-code[## Example continued

-   Recall that regression trees imply 'step function' estimation of trend.

-   The more complex the tree, the more complex the step function!
]

.right-plot[
```{r, echo=FALSE}
toy.rp.1 <- rpart(y ~ x, cp=0.1, data=toytree)
toy.rp.2 <- rpart(y ~ x, cp=0.01, data=toytree)
toy.rp.3 <- rpart(y ~ x, cp=0.000001, data=toytree, minsplit=3)
par(mar=c(3,1,1,1))
par(mfrow=c(3,2))
plot(toy.rp.1,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.1),lwd=2)
plot(toy.rp.2,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.2),lwd=2)
plot(toy.rp.3,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.3),lwd=2)
```
]

---

## Model Complexity

-   We can think about the bias-variance trade off in terms of model
    complexity.

-   Low complexity -- high bias -- low variability.

-   High complexity -- low bias -- high variability.

-   This motivates the introduction of *complexity parameter* `cp`.

-   `cp` specifies the minimum improvement in model fit that warrants
    inclusion of a new split in tree.

-   Implies tree growth ceases when no split produces an improvement of
    at least `cp`.

-   `cp` is the relative improvement in residual sum of squares.

-   Specified by `cp` argument of `rpart`, default `cp = 0.01`.

---

.left-code[## Regression Trees of different complexities

```{r}
wage.rp.2 <- rpart(WAGE ~ .,
                   data=wage.train,
                   cp=0.1)

wage.rp.3 <- rpart(WAGE ~ .,
                   data=wage.train,
                   cp=0.001)
```

There is a clear difference in complexity here!
]

.right-plot[
```{r, echo=FALSE}
par(mfrow=c(2,1), mar=c(0,0,1,0))
plot(wage.rp.2,compress=T,margin=0.1)
text(wage.rp.2,cex=0.7)
title("cp=0.1")
plot(wage.rp.3,compress=T,margin=0.1)
text(wage.rp.3,cex=0.7)
title("cp=0.001")
```
]

---

## MSE of Prediction for Models

```{r}
wage.rp.pred <- wage.test %>%
  bind_cols(
    .pred1 = predict(wage.rp, newdata=wage.test),
    .pred2 = predict(wage.rp.2, newdata=wage.test),
    .pred3 = predict(wage.rp.3, newdata=wage.test)
  )
wage.rp.pred %>%
  summarise(MSE.1 = mean((.pred1 - WAGE)^2),
            MSE.2 = mean((.pred2 - WAGE)^2),
            MSE.3 = mean((.pred3 - WAGE)^2))
```

Prediction using `cp=0.001` is better than the default value of `cp=0.01` in this case.

---

## Validation

-   The default value of $cp=0.01$ is only a rule-of-thumb.

-   Can often get better predictions by adjusting this parameter.

-   No use tuning on training data - will always select the most complex
    tree (overfitting).

-   One approach is to divide available data into training and
    *validation* datasets.

    -   Usually training set larger - e.g. training to validation size
        ratio of 3:1, or even 9:1.

-   Then use the training data for growing the tree.

-   Predictive accuracy is then assessed by predicting on the validation dataset.

-   Pick the value of `cp` to that minimizes prediction error.

---

## Cross-Validation

-   Validation results will depend on the particular decomposition into training
    and validation subsets.

-   Cross-validation addresses this issue.

-   The idea is that we split the data into equally size blocks
    (subgroups).

-   Each block in turn being set aside as the validation data, with
    remaining blocks combining to form the training set.

-   The prediction error is then averaged over the results for each validation set.

-   Common to use *10-fold* cross-validation when the data are split into 10
    blocks (so 90% to 10% training to validation ratio at each stage).

    -   Extreme version is 'leave-one-out' cross-validation.

-   Can be run for different values of `cp`.

---

.left-code[## Cross-Validation for the Wage Data Trees

A table of model characteristics for different values of `cp` can be
obtained using the command `printcp`.

```{r wagecp, eval=FALSE}
printcp(wage.rp.3)
```

The `xerror` column contains cross-validation estimates of the
(relative) prediction error.

`xstd` is the standard error (i.e. an estimate of the uncertainty)
for these cross-validation estimates.
]

.small-font.right-plot[
```{r, ref.label='wagecp', echo=FALSE}
```
]

--
.left-code[**No real evidence to prefer any tree past 1 split!**
]

---

class: middle,inverse

# Forests

---
