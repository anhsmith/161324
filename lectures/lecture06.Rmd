---
title: 'Lecture 6'
subtitle: 'Neural networks'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

wage.train <- read_csv("../data/wage-train.csv")
wage.test  <- read_csv("../data/wage-test.csv")
```

.left-code[
## Neural Networks

-   An artificial neural network (or just 'neural network') is
    a type of prediction model based on a crude representation
    of a human brain.

-   The model comprises a highly interconnected network of
    'neurons' which 'fire' (output) only if the level of input
    is sufficiently high.
]

.right-plot[
```{r, echo=FALSE}
knitr::include_graphics("graphics/brain.jpg")
```
]

---

.left-code[
## Neural Networks

An artificial feed-forward neural network:

-   Comprises ordered sequence of layers of neurons.

-   Each node connects to all nodes in next layer.

-   Input layer (predictor nodes) feeds to hidden layer
    ('derived features' $z$), and then to output node (target
    variable).
]

.right-plot[
```{r, echo=FALSE}
par(mar=c(0,0,0,0))
plot(c(1,9),c(0,11),type="n",axes=F,xlab="",ylab="")
symbols(c(2,2,2,2,5,5,5,8),c(1,3.67,6.33,9,2,5,8,5),circle=rep(0.5,8),inches=F,add=T)
arrows(2.5,1,4.5,2,length=0.1)
arrows(2.5,1,4.5,5,length=0.1)
arrows(2.5,1,4.5,8,length=0.1)
arrows(2.5,3.67,4.5,2,length=0.1)
arrows(2.5,3.67,4.5,5,length=0.1)
arrows(2.5,3.67,4.5,8,length=0.1)
arrows(2.5,6.33,4.5,2,length=0.1)
arrows(2.5,6.33,4.5,5,length=0.1)
arrows(2.5,6.33,4.5,8,length=0.1)
arrows(2.5,9,4.5,2,length=0.1)
arrows(2.5,9,4.5,5,length=0.1)
arrows(2.5,9,4.5,8,length=0.1)
arrows(5.5,2,7.5,5,length=0.1)
arrows(5.5,5,7.5,5,length=0.1)
arrows(5.5,8,7.5,5,length=0.1)
text(2,9,expression(x[1]),cex=1.5)
text(2,6.33,expression(x[2]),cex=1.5)
text(2,3.67,expression(x[3]),cex=1.5)
text(2,1,expression(x[4]),cex=1.5)
text(5,8,expression(z[1]),cex=1.5)
text(5,5,expression(z[2]),cex=1.5)
text(5,2,expression(z[3]),cex=1.5)
text(8,5,expression(hat(y)),cex=1.5)
text(2,10.2,"Inputs",cex=1.5)
text(5,10.2,"Hidden layer",cex=1.5)
text(8,10.2,"Output",cex=1.5)
```
]
---

## Building a Neural Network

-   To implement a neural network we must:

    1.  decide on the number, $M$, of nodes in the hidden layer;

    2.  devise functions to relate the *derived features*
        $z_1, \ldots, z_M$ (i.e. the values in the hidden layer) to the
        input variables;

    3.  choose a function to describe the predicted target $\hat y$ in
        terms of $z_1, \ldots, z_M$.

-   Neural networks so complex that little theory exists to help with
    these choices.

-   Rule of thumb: try $\lceil p/2 \rceil$ hidden nodes.

    -   $\lceil p/2 \rceil$ is smallest integer at least as large as
        $p/2$; e.g. $\lceil 7/2 \rceil = 4$.

    -   $p$ is effective number of predictors, where a factor on $K$
        levels contributes $K-1$ predictors (through coding to dummy
        variables).

    -   E.g. If there are 3 numerical predictors and one factor on 4
        levels, then $p = 3+ (4-1) = 6$.
---

## Building a Neural Network

-   The function that relates the derived features to the inputs
    (i.e. predictors) is called the *activation function*.

    -   Denote by $\phi$.

-   $\phi$ operates on a linear combination of the predictors.

-   Hence $z_k = \phi(v_k)$ where
    $$v_k = \alpha_{0k} + \alpha_{1k} x_1 + \cdots + \alpha_{pk} x_p~~~~(k=1,\ldots,M)$$

-   Predicted target $\hat y$ is derived as a linear combination of the
    hidden features:
    $$\hat y = \beta_0 + \beta_1 z_1 + \cdots + \beta_M z_M.$$

---

.left-code[
## Sigmoid Activation Function

-   Common choice of $\phi$ is the sigmoid function
    $\phi(v) = \tfrac{1}{1 + e^{-v}}$.

-   Note that large input $v > 4$ returns almost one; small input $v < -4$
    returns almost zero.
]

.right-plot[
```{r, echo=FALSE}
ggplot() +
  geom_function(fun = ~1/(1+exp(-.x)), xlim=c(-5, 5)) +
  labs(x = 'v',
       y = expression(phi(v)))
```
]

---

## Fitting A Neural Network

-   Neural network defined by large number of parameters:
    $\alpha_{01}, \alpha_{11}, \ldots, \beta_{M}$.

-   These are often referred to as *weights* for the neural net.

-   They must be estimated by fitting the model to training data.

-   We aim to select the weights that minimize the residual sum of
    squares for predictions from the neural network.

-   Not straightforward because of the complexity of the model.

    -   Consider the sheer number of parameters that need to be
        estimated.

-   In practice neural networks are fitted using a so-called *back
    propagation algorithm*.

-   This involves a complicated search over possible weights, and can be
    dependent on 'initial values' (i.e. search start point).

---

## Implementing Neural Networks in R

-   R function for fitting a neural network is `nnet`.

-   This is part of the `nnet` package which must be pre-loaded.

-   The syntax to fit a neural network is of the form

        nnet(y ~ x1 + x2 + x3, data=mydata, size=2, linout=T)

-   The specification of the model formula and data frame is familiar.

-   `size` argument specifies the number of nodes in the hidden layer.
    It has no default value.

-   The argument `linout` is logical, and indicates whether the
    relationship between the prediction (output $\hat y$) and the
    derived features is linear or not.

    -   Default setting for `linout` is `FALSE`, when `nnet` produces
        output constrained to internal $[0,1]$. (Good as classification
        probability.)

    -   For prediction problems we must set `linout=T`.

---

## Fitting Neural Networks in R

-   `nnet` has optional arguments controlling model fitting.

-   `decay` controls weight updating rate in back propagation algorithm.

    -   Can have quite a significant impact on the results.

    -   Default value is zero, but setting `decay` to a value such as
        $0.01$ or $0.001$ often a better choice.

-   The model fitting algorithm used by `nnet` needs initial values for
    the weights.

-   By default these randomly chosen on the interval $[-0.5,0.5]$.

-   Final values of the weights can be quite dependent upon the choice
    of initial values,

    -   Means fitted nets can vary even when rerunning same code.

    -   Can suppress this variation by setting the random number seed
        with the `set.seed()` command.

---

## Neural Networks for the Wage Data

-   We will train a neural network on wage data.

-   For number of hidden nodes to use initially, need to count number of
    effective predictors.

-   Data frame has 5 numerical predictors:

    -   `EDU`, `SOUTH`, `EXP`, `UNION`, and `AGE`;

-   There are 5 factors:

    -   `SEX` (with 2 levels), `RACE` (3 levels), `OCCUP` (6 levels),
        `SECTOR` (3 levels) and `MARRIAGE` (2 levels).

-   Gives effective number predictors $p = 5 + 1 + 2 + 5 + 2 + 1 = 16$.

-   Suggests we try $p/2 = 8$ nodes in the hidden layer.

-   Will obtain predictions using `predict` function applied to trained
    network.

---

## Neural Networks for the Wage Data

```{r}
library(nnet)
set.seed(1069)
wage.nn.1 <- nnet(WAGE ~ .,size=8, data=wage.train, linout=TRUE)
wage.test %>%
  bind_cols(
    .pred = predict(wage.nn.1, newdata=wage.test)
  ) %>%
  summarise(MSE = mean((.pred - WAGE)^2))
```

---

## Example continuedMore R code

```{r}
set.seed(1069)
wage.nn.2 <- nnet(WAGE ~ .,size=5, data=wage.train, linout=TRUE)
set.seed(1069)
wage.nn.3 <- nnet(WAGE ~ .,size=8, data=wage.train, linout=TRUE, decay=0.01)
set.seed(1069)
wage.nn.4 <- nnet(WAGE ~ .,size=5, data=wage.train, linout=TRUE, decay=0.01)
wage.test %>%
  bind_cols(
    pred1 = predict(wage.nn.1, newdata=wage.test),
    pred2 = predict(wage.nn.2, newdata=wage.test),
    pred3 = predict(wage.nn.3, newdata=wage.test),
    pred4 = predict(wage.nn.4, newdata=wage.test),
  ) %>%
  summarise(across(starts_with('pred'), ~mean((. - WAGE)^2)))
```

---

## Example continuedCommentary on R code

-   We reinitialize random number generator with `set.seed` before
    fitting each model so as to improve comparability between models.

-   The first model has 8 hidden nodes. How many weights?

    -   $p+1 = 17$ weights per hidden node (number of $\alpha$s);

    -   $8+1=9$ weights for output node (number of $\beta$s);

    -   Net hence has $17\times 8 + 9 = 145$ weights to be determined.

-   Mean square prediction error for this model is $\MSE = 22.3$.

-   Second neural network:

    -   5 hidden nodes, (so 91 weights to be determined).

    -   We get $\MSE = 23.7$.

-   Third and fourth nets differ from the first and second only in that
    we specify a value of $0.01$ for the `decay` argument.

-   Results in small changes to prediction errors (better in one case,
    worse in other).

---

## Summary of Neural Network Prediction

-   Neural networks are extremely flexible for prediction.

-   Complexity depends on number of hidden nodes.

    -   Bias-variance trade-off applies in theory: increasing number of
        hidden nodes will reduce bias but increase variance.

    -   In practice the picture is confused by the difficulties in model
        fitting.

-   Training neural nets is a complex problem.

    -   Fitting methods less stable than for linear models and
        regression trees.

    -   Numerical issues can arise when predictors of (very) different
        scales.

    -   Pre-scaling of predictors can be advisable.

---

class: middle,inverse

# A prediction example

---
